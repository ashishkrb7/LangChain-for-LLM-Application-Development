{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"LangChain for LLM Application Development","text":"<p>Documentation</p>"},{"location":"#project-overview","title":"Project Overview","text":""},{"location":"#getting-started","title":"Getting Started","text":"<ul> <li> <p>Install Miniconda from https:/.conda.io/en/latest/miniconda.html#windows-installers (for python)</p> </li> <li> <p>After Anaconda installation, go to search and run Anaconda Prompt and create virtual environment using following commands</p> <p><code>conda create -y -n gpt python=3.11.0</code></p> </li> <li> <p>Activate the conda environment</p> <p><code>conda activate gpt</code></p> </li> <li> <p>Clone the repository to your local machine. </p> <p><code>git clone https://github.com/ashishkrb7/LangChain-for-LLM-Application-Development.git</code> </p> </li> <li> <p>Go to working directory</p> <p><code>cd LangChain-for-LLM-Application-Development</code></p> </li> <li> <p>Install the required dependencies using </p> <p><code>python -m pip install -r requirements.txt</code></p> </li> <li> <p>Go to notebook folder</p> <p><code>cd docs/notebooks</code></p> </li> <li> <p>Create .env file. It should contain following information</p> <pre><code>api_type = \napi_base = \napi_version = \nOPENAI_API_KEY = \n</code></pre> </li> </ul>"},{"location":"#conclusion","title":"Conclusion","text":""},{"location":"Agents/","title":"LangChain: Agents","text":""},{"location":"Agents/#outline","title":"Outline:","text":"<ul> <li>Using built in LangChain tools: DuckDuckGo search and Wikipedia</li> <li>Defining your own tools</li> </ul> <pre><code>import os\n\nfrom dotenv import load_dotenv, find_dotenv\nfrom langchain.chat_models import AzureChatOpenAI\n_ = load_dotenv(find_dotenv()) # read local .env file\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n</code></pre> <pre><code># Set OpenAI API key\nos.environ[\"OPENAI_API_TYPE\"] = os.getenv(\"api_type\")\nos.environ[\"OPENAI_API_BASE\"] = os.getenv(\"api_base\")\nos.environ[\"OPENAI_API_VERSION\"] = os.getenv(\"api_version\")\nos.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n</code></pre> <pre><code>llm = AzureChatOpenAI(deployment_name=\"chatgpt-gpt35-turbo\",model_name=\"gpt-35-turbo\",temperature=0.0)\n</code></pre>"},{"location":"Agents/#built-in-langchain-tools","title":"Built-in LangChain tools","text":"<pre><code>#!pip install -U wikipedia\n</code></pre> <pre><code>from langchain.agents.agent_toolkits import create_python_agent\nfrom langchain.agents import load_tools, initialize_agent\nfrom langchain.agents import AgentType\nfrom langchain.tools.python.tool import PythonREPLTool\nfrom langchain.python import PythonREPL\nfrom langchain.chat_models import ChatOpenAI\n</code></pre> <pre><code>tools = load_tools([\"llm-math\",\"wikipedia\"], llm=llm)\n</code></pre> <pre><code>agent= initialize_agent(\n    tools, \n    llm, \n    agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n    handle_parsing_errors=True,\n    verbose = True)\n</code></pre> <pre><code>agent(\"What is the 25% of 300?\")\n</code></pre> <pre><code>Error in on_chain_start callback: 'name'\n\n\n\u001b[32;1m\u001b[1;3mThought: I need to use a calculator to find the answer to this math problem.\n\nAction:\n```\n{\n  \"action\": \"Calculator\",\n  \"action_input\": \"300*0.25\"\n}\n```\n\n\u001b[0m\nObservation: \u001b[36;1m\u001b[1;3mAnswer: 75.0\u001b[0m\nThought:\u001b[32;1m\u001b[1;3mI have successfully calculated the answer to the math problem.\n\nFinal Answer: 75.0\u001b[0m\n\n\u001b[1m&gt; Finished chain.\u001b[0m\n\n\n\n\n\n{'input': 'What is the 25% of 300?', 'output': '75.0'}\n</code></pre> <pre><code>question = \"Tom M. Mitchell is an American computer scientist \\\nand the Founders University Professor at Carnegie Mellon University (CMU)\\\nwhat book did he write?\"\nresult = agent(question) \n</code></pre> <pre><code>Error in on_chain_start callback: 'name'\n\n\n\u001b[32;1m\u001b[1;3mThought: I should use Wikipedia to find the answer to this question.\n\nAction:\n```\n{\n  \"action\": \"Wikipedia\",\n  \"action_input\": \"Tom M. Mitchell\"\n}\n```\n\n\u001b[0m\nObservation: \u001b[33;1m\u001b[1;3mPage: Tom M. Mitchell\nSummary: Tom Michael Mitchell (born August 9, 1951) is an American computer scientist and the Founders University Professor at Carnegie Mellon University (CMU). He is a founder and former Chair of the Machine Learning Department at CMU. Mitchell is known for his contributions to the advancement of machine learning, artificial intelligence, and cognitive neuroscience and is the author of the textbook Machine Learning. He is a member of the United States National Academy of Engineering since 2010. He is also a Fellow of the American Academy of Arts and Sciences, the American Association for the Advancement of Science and a Fellow and past President of the Association for the Advancement of Artificial Intelligence. In October 2018, Mitchell was appointed as the Interim Dean of the School of Computer Science at Carnegie Mellon.\n\nPage: Tom Mitchell (Australian footballer)\nSummary: Thomas Mitchell (born 31 May 1993) is a professional Australian rules footballer playing for the Collingwood Football Club in the Australian Football League (AFL). He previously played for the Sydney Swans from 2012 to 2016, and the Hawthorn Football Club between 2017 and 2022. Mitchell won the Brownlow Medal as the league's best and fairest player in 2018 and set the record for the most disposals in a VFL/AFL match, accruing 54 in a game against Collingwood during that season.\u001b[0m\nThought:\u001b[32;1m\u001b[1;3mThe book that Tom M. Mitchell wrote is \"Machine Learning\".\n\nAction:\n```\n{\n  \"action\": \"Wikipedia\",\n  \"action_input\": \"Machine Learning (book)\"\n}\n```\n\n\u001b[0m\nObservation: \u001b[33;1m\u001b[1;3mPage: Machine learning\nSummary: Machine learning (ML) is a branch of artificial intelligence that leverages data to improve computer performance by giving machines the ability to \"learn\".Machine learning algorithms build a model based on sample data, known as training data, in order to make predictions or decisions without being explicitly programmed to do so. Machine learning algorithms are used in a wide variety of applications, such as in medicine, email filtering, speech recognition, agriculture, and computer vision, where it is difficult or unfeasible to develop conventional algorithms to perform the needed tasks.A subset of machine learning is closely related to computational statistics, which focuses on making predictions using computers, but not all machine learning is statistical learning. The study of mathematical optimization delivers methods, theory and application domains to the field of machine learning. Data mining is a related field of study, focusing on exploratory data analysis through unsupervised learning.Some implementations of machine learning use data and artificial neural networks in a way that mimics the working of a biological brain.In its application across business problems, machine learning is also referred to as predictive analytics.\n\nPage: Quantum machine learning\nSummary: Quantum machine learning is the integration of quantum algorithms within machine learning programs. The most common use of the term refers to machine learning algorithms for the analysis of classical data executed on a quantum computer, i.e. quantum-enhanced machine learning. While machine learning algorithms are used to compute immense quantities of data, quantum machine learning utilizes qubits and quantum operations or specialized quantum systems to improve computational speed and data storage done by algorithms in a program. This includes hybrid methods that involve both classical and quantum processing, where computationally difficult subroutines are outsourced to a quantum device. These routines can be more complex in nature and executed faster on a quantum computer. Furthermore, quantum algorithms can be used to analyze quantum states instead of classical data. Beyond quantum computing, the term \"quantum machine learning\" is also associated with classical machine learning methods applied to data generated from quantum experiments (i.e. machine learning of quantum systems), such as learning the phase transitions of a quantum system or creating new quantum experiments. Quantum machine learning also extends to a branch of research that explores methodological and structural similarities between certain physical systems and learning systems, in particular neural networks. For example, some mathematical and numerical techniques from quantum physics are applicable to classical deep learning and vice versa. Furthermore, researchers investigate more abstract notions of learning theory with respect to quantum information, sometimes referred to as \"quantum learning theory\".\n\n\n\nPage: Timeline of machine learning\nSummary: This page is a timeline of machine learning. Major discoveries, achievements, milestones and other major events in machine learning are included.\u001b[0m\nThought:\u001b[32;1m\u001b[1;3mTom M. Mitchell wrote the book \"Machine Learning\".\n\nFinal Answer: \"Machine Learning\".\u001b[0m\n\n\u001b[1m&gt; Finished chain.\u001b[0m\n</code></pre>"},{"location":"Agents/#python-agent","title":"Python Agent","text":"<pre><code>agent = create_python_agent(\n    llm,\n    tool=PythonREPLTool(),\n    verbose=True\n)\n</code></pre> <pre><code>customer_list = [[\"Harrison\", \"Chase\"], \n                 [\"Lang\", \"Chain\"],\n                 [\"Dolly\", \"Too\"],\n                 [\"Elle\", \"Elem\"], \n                 [\"Geoff\",\"Fusion\"], \n                 [\"Trance\",\"Former\"],\n                 [\"Jen\",\"Ayai\"]\n                ]\n</code></pre> <pre><code>agent.run(f\"\"\"Sort these customers by \\\nlast name and then first name \\\nand print the output: {customer_list}\"\"\") \n</code></pre> <pre><code>Error in on_chain_start callback: 'name'\n\n\n\u001b[32;1m\u001b[1;3mI can use the sorted() function to sort the list of customers by last name and then first name. I will need to provide a key function to sorted() that returns a tuple of the last name and first name in that order.\nAction: Python REPL\nAction Input:\n```\ncustomers = [['Harrison', 'Chase'], ['Lang', 'Chain'], ['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Jen', 'Ayai']]\nsorted_customers = sorted(customers, key=lambda x: (x[1], x[0]))\nfor customer in sorted_customers:\n    print(customer)\n```\u001b[0m\nObservation: \u001b[36;1m\u001b[1;3m['Jen', 'Ayai']\n['Lang', 'Chain']\n['Harrison', 'Chase']\n['Elle', 'Elem']\n['Trance', 'Former']\n['Geoff', 'Fusion']\n['Dolly', 'Too']\n\u001b[0m\nThought:\u001b[32;1m\u001b[1;3mThe customers have been sorted by last name and then first name, and the output has been printed. \nFinal Answer: [['Jen', 'Ayai'], ['Lang', 'Chain'], ['Harrison', 'Chase'], ['Elle', 'Elem'], ['Trance', 'Former'], ['Geoff', 'Fusion'], ['Dolly', 'Too']]\u001b[0m\n\n\u001b[1m&gt; Finished chain.\u001b[0m\n\n\n\n\n\n\"[['Jen', 'Ayai'], ['Lang', 'Chain'], ['Harrison', 'Chase'], ['Elle', 'Elem'], ['Trance', 'Former'], ['Geoff', 'Fusion'], ['Dolly', 'Too']]\"\n</code></pre>"},{"location":"Agents/#view-detailed-outputs-of-the-chains","title":"View detailed outputs of the chains","text":"<pre><code>import langchain\nlangchain.debug=True\nagent.run(f\"\"\"Sort these customers by \\\nlast name and then first name \\\nand print the output: {customer_list}\"\"\") \nlangchain.debug=False\n</code></pre> <pre><code>\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:AgentExecutor] Entering Chain run with input:\n\u001b[0m{\n  \"input\": \"Sort these customers by last name and then first name and print the output: [['Harrison', 'Chase'], ['Lang', 'Chain'], ['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Jen', 'Ayai']]\"\n}\n\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:AgentExecutor &gt; 2:chain:LLMChain] Entering Chain run with input:\n\u001b[0m{\n  \"input\": \"Sort these customers by last name and then first name and print the output: [['Harrison', 'Chase'], ['Lang', 'Chain'], ['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Jen', 'Ayai']]\",\n  \"agent_scratchpad\": \"\",\n  \"stop\": [\n    \"\\nObservation:\",\n    \"\\n\\tObservation:\"\n  ]\n}\n\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:AgentExecutor &gt; 2:chain:LLMChain &gt; 3:llm:AzureChatOpenAI] Entering LLM run with input:\n\u001b[0m{\n  \"prompts\": [\n    \"Human: You are an agent designed to write and execute python code to answer questions.\\nYou have access to a python REPL, which you can use to execute python code.\\nIf you get an error, debug your code and try again.\\nOnly use the output of your code to answer the question. \\nYou might know the answer without running any code, but you should still run the code to get the answer.\\nIf it does not seem like you can write code to answer the question, just return \\\"I don't know\\\" as the answer.\\n\\n\\nPython REPL: A Python shell. Use this to execute python commands. Input should be a valid python command. If you want to see the output of a value, you should print it out with `print(...)`.\\n\\nUse the following format:\\n\\nQuestion: the input question you must answer\\nThought: you should always think about what to do\\nAction: the action to take, should be one of [Python REPL]\\nAction Input: the input to the action\\nObservation: the result of the action\\n... (this Thought/Action/Action Input/Observation can repeat N times)\\nThought: I now know the final answer\\nFinal Answer: the final answer to the original input question\\n\\nBegin!\\n\\nQuestion: Sort these customers by last name and then first name and print the output: [['Harrison', 'Chase'], ['Lang', 'Chain'], ['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Jen', 'Ayai']]\\nThought:\"\n  ]\n}\n\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:AgentExecutor &gt; 2:chain:LLMChain &gt; 3:llm:AzureChatOpenAI] [4.32s] Exiting LLM run with output:\n\u001b[0m{\n  \"generations\": [\n    [\n      {\n        \"text\": \"I can use the sorted() function to sort the list of customers by last name and then first name. I will need to provide a key function to sorted() that returns a tuple of the last name and first name in that order.\\nAction: Python REPL\\nAction Input:\\n```\\ncustomers = [['Harrison', 'Chase'], ['Lang', 'Chain'], ['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Jen', 'Ayai']]\\nsorted_customers = sorted(customers, key=lambda x: (x[1], x[0]))\\nfor customer in sorted_customers:\\n    print(customer)\\n```\",\n        \"generation_info\": null,\n        \"message\": {\n          \"content\": \"I can use the sorted() function to sort the list of customers by last name and then first name. I will need to provide a key function to sorted() that returns a tuple of the last name and first name in that order.\\nAction: Python REPL\\nAction Input:\\n```\\ncustomers = [['Harrison', 'Chase'], ['Lang', 'Chain'], ['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Jen', 'Ayai']]\\nsorted_customers = sorted(customers, key=lambda x: (x[1], x[0]))\\nfor customer in sorted_customers:\\n    print(customer)\\n```\",\n          \"additional_kwargs\": {},\n          \"example\": false\n        }\n      }\n    ]\n  ],\n  \"llm_output\": {\n    \"token_usage\": {\n      \"completion_tokens\": 144,\n      \"prompt_tokens\": 327,\n      \"total_tokens\": 471\n    },\n    \"model_name\": \"gpt-35-turbo\"\n  },\n  \"run\": null\n}\n\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:AgentExecutor &gt; 2:chain:LLMChain] [4.32s] Exiting Chain run with output:\n\u001b[0m{\n  \"text\": \"I can use the sorted() function to sort the list of customers by last name and then first name. I will need to provide a key function to sorted() that returns a tuple of the last name and first name in that order.\\nAction: Python REPL\\nAction Input:\\n```\\ncustomers = [['Harrison', 'Chase'], ['Lang', 'Chain'], ['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Jen', 'Ayai']]\\nsorted_customers = sorted(customers, key=lambda x: (x[1], x[0]))\\nfor customer in sorted_customers:\\n    print(customer)\\n```\"\n}\n\u001b[32;1m\u001b[1;3m[tool/start]\u001b[0m \u001b[1m[1:chain:AgentExecutor &gt; 4:tool:Python REPL] Entering Tool run with input:\n\u001b[0m\"```\ncustomers = [['Harrison', 'Chase'], ['Lang', 'Chain'], ['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Jen', 'Ayai']]\nsorted_customers = sorted(customers, key=lambda x: (x[1], x[0]))\nfor customer in sorted_customers:\n    print(customer)\n```\"\n\u001b[36;1m\u001b[1;3m[tool/end]\u001b[0m \u001b[1m[1:chain:AgentExecutor &gt; 4:tool:Python REPL] [0.467ms] Exiting Tool run with output:\n\u001b[0m\"['Jen', 'Ayai']\n['Lang', 'Chain']\n['Harrison', 'Chase']\n['Elle', 'Elem']\n['Trance', 'Former']\n['Geoff', 'Fusion']\n['Dolly', 'Too']\"\n\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:AgentExecutor &gt; 5:chain:LLMChain] Entering Chain run with input:\n\u001b[0m{\n  \"input\": \"Sort these customers by last name and then first name and print the output: [['Harrison', 'Chase'], ['Lang', 'Chain'], ['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Jen', 'Ayai']]\",\n  \"agent_scratchpad\": \"I can use the sorted() function to sort the list of customers by last name and then first name. I will need to provide a key function to sorted() that returns a tuple of the last name and first name in that order.\\nAction: Python REPL\\nAction Input:\\n```\\ncustomers = [['Harrison', 'Chase'], ['Lang', 'Chain'], ['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Jen', 'Ayai']]\\nsorted_customers = sorted(customers, key=lambda x: (x[1], x[0]))\\nfor customer in sorted_customers:\\n    print(customer)\\n```\\nObservation: ['Jen', 'Ayai']\\n['Lang', 'Chain']\\n['Harrison', 'Chase']\\n['Elle', 'Elem']\\n['Trance', 'Former']\\n['Geoff', 'Fusion']\\n['Dolly', 'Too']\\n\\nThought:\",\n  \"stop\": [\n    \"\\nObservation:\",\n    \"\\n\\tObservation:\"\n  ]\n}\n\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:AgentExecutor &gt; 5:chain:LLMChain &gt; 6:llm:AzureChatOpenAI] Entering LLM run with input:\n\u001b[0m{\n  \"prompts\": [\n    \"Human: You are an agent designed to write and execute python code to answer questions.\\nYou have access to a python REPL, which you can use to execute python code.\\nIf you get an error, debug your code and try again.\\nOnly use the output of your code to answer the question. \\nYou might know the answer without running any code, but you should still run the code to get the answer.\\nIf it does not seem like you can write code to answer the question, just return \\\"I don't know\\\" as the answer.\\n\\n\\nPython REPL: A Python shell. Use this to execute python commands. Input should be a valid python command. If you want to see the output of a value, you should print it out with `print(...)`.\\n\\nUse the following format:\\n\\nQuestion: the input question you must answer\\nThought: you should always think about what to do\\nAction: the action to take, should be one of [Python REPL]\\nAction Input: the input to the action\\nObservation: the result of the action\\n... (this Thought/Action/Action Input/Observation can repeat N times)\\nThought: I now know the final answer\\nFinal Answer: the final answer to the original input question\\n\\nBegin!\\n\\nQuestion: Sort these customers by last name and then first name and print the output: [['Harrison', 'Chase'], ['Lang', 'Chain'], ['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Jen', 'Ayai']]\\nThought:I can use the sorted() function to sort the list of customers by last name and then first name. I will need to provide a key function to sorted() that returns a tuple of the last name and first name in that order.\\nAction: Python REPL\\nAction Input:\\n```\\ncustomers = [['Harrison', 'Chase'], ['Lang', 'Chain'], ['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Jen', 'Ayai']]\\nsorted_customers = sorted(customers, key=lambda x: (x[1], x[0]))\\nfor customer in sorted_customers:\\n    print(customer)\\n```\\nObservation: ['Jen', 'Ayai']\\n['Lang', 'Chain']\\n['Harrison', 'Chase']\\n['Elle', 'Elem']\\n['Trance', 'Former']\\n['Geoff', 'Fusion']\\n['Dolly', 'Too']\\n\\nThought:\"\n  ]\n}\n\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:AgentExecutor &gt; 5:chain:LLMChain &gt; 6:llm:AzureChatOpenAI] [2.27s] Exiting LLM run with output:\n\u001b[0m{\n  \"generations\": [\n    [\n      {\n        \"text\": \"The customers have been sorted by last name and then first name.\\nFinal Answer: [['Jen', 'Ayai'], ['Lang', 'Chain'], ['Harrison', 'Chase'], ['Elle', 'Elem'], ['Trance', 'Former'], ['Geoff', 'Fusion'], ['Dolly', 'Too']]\",\n        \"generation_info\": null,\n        \"message\": {\n          \"content\": \"The customers have been sorted by last name and then first name.\\nFinal Answer: [['Jen', 'Ayai'], ['Lang', 'Chain'], ['Harrison', 'Chase'], ['Elle', 'Elem'], ['Trance', 'Former'], ['Geoff', 'Fusion'], ['Dolly', 'Too']]\",\n          \"additional_kwargs\": {},\n          \"example\": false\n        }\n      }\n    ]\n  ],\n  \"llm_output\": {\n    \"token_usage\": {\n      \"completion_tokens\": 67,\n      \"prompt_tokens\": 526,\n      \"total_tokens\": 593\n    },\n    \"model_name\": \"gpt-35-turbo\"\n  },\n  \"run\": null\n}\n\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:AgentExecutor &gt; 5:chain:LLMChain] [2.27s] Exiting Chain run with output:\n\u001b[0m{\n  \"text\": \"The customers have been sorted by last name and then first name.\\nFinal Answer: [['Jen', 'Ayai'], ['Lang', 'Chain'], ['Harrison', 'Chase'], ['Elle', 'Elem'], ['Trance', 'Former'], ['Geoff', 'Fusion'], ['Dolly', 'Too']]\"\n}\n\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:AgentExecutor] [6.60s] Exiting Chain run with output:\n\u001b[0m{\n  \"output\": \"[['Jen', 'Ayai'], ['Lang', 'Chain'], ['Harrison', 'Chase'], ['Elle', 'Elem'], ['Trance', 'Former'], ['Geoff', 'Fusion'], ['Dolly', 'Too']]\"\n}\n</code></pre>"},{"location":"Agents/#define-your-own-tool","title":"Define your own tool","text":"<pre><code># !pip install DateTime\n</code></pre> <pre><code>from langchain.agents import tool\nfrom datetime import date\n</code></pre> <pre><code>@tool\ndef time(text: str) -&gt; str:\n\"\"\"Returns todays date, use this for any \\\n    questions related to knowing todays date. \\\n    The input should always be an empty string, \\\n    and this function will always return todays \\\n    date - any date mathmatics should occur \\\n    outside this function.\"\"\"\n    return str(date.today())\n</code></pre> <pre><code>agent= initialize_agent(\n    tools + [time], \n    llm, \n    agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n    handle_parsing_errors=True,\n    verbose = True)\n</code></pre> <p>Note: </p> <p>The agent will sometimes come to the wrong conclusion (agents are a work in progress!). </p> <p>If it does, please try running it again.</p> <pre><code>try:\n    result = agent(\"whats the date today?\") \nexcept: \n    print(\"exception on external access\")\n</code></pre> <pre><code>Error in on_chain_start callback: 'name'\n\n\n\u001b[32;1m\u001b[1;3mQuestion: whats the date today?\nThought: I can use the `time` tool to get today's date\nAction:\n```\n{\n  \"action\": \"time\",\n  \"action_input\": \"\"\n}\n```\n\u001b[0m\nObservation: \u001b[38;5;200m\u001b[1;3m2023-07-02\u001b[0m\nThought:\u001b[32;1m\u001b[1;3mI have successfully used the `time` tool to get today's date.\nFinal Answer: Today's date is 2023-07-02.\u001b[0m\n\n\u001b[1m&gt; Finished chain.\u001b[0m\n</code></pre>"},{"location":"Chains/","title":"Chains in LangChain","text":""},{"location":"Chains/#outline","title":"Outline","text":"<ul> <li>LLMChain</li> <li>Sequential Chains</li> <li>SimpleSequentialChain</li> <li>SequentialChain</li> <li>Router Chain</li> </ul> <pre><code>import warnings\nwarnings.filterwarnings('ignore')\n</code></pre> <pre><code>import os\n\nfrom dotenv import load_dotenv, find_dotenv\n_ = load_dotenv(find_dotenv()) # read local .env file\n</code></pre> <pre><code>#!pip install pandas\n</code></pre> <pre><code>import pandas as pd\ndf = pd.read_csv('data.csv',delimiter=\"|\")\n</code></pre> <pre><code>df.head()\n</code></pre> Product Review 0 Queen Size Sheet Set I ordered a king size set. My only criticism w... 1 Waterproof Phone Pouch I loved the waterproof sac, although the openi... 2 Luxury Air Mattress This mattress had a small hole in the top of i... 3 Pillows Insert This is the best throw pillow fillers on Amazo... 4 Milk Frother Handheld I loved this product. But they only seem to la..."},{"location":"Chains/#llmchain","title":"LLMChain","text":"<pre><code>from langchain.chat_models import ChatOpenAI\nfrom langchain.prompts import ChatPromptTemplate\nfrom langchain.chains import LLMChain\nfrom langchain.chat_models import AzureChatOpenAI\n</code></pre> <pre><code># Set OpenAI API key\nos.environ[\"OPENAI_API_TYPE\"] = os.getenv(\"api_type\")\nos.environ[\"OPENAI_API_BASE\"] = os.getenv(\"api_base\")\nos.environ[\"OPENAI_API_VERSION\"] = os.getenv(\"api_version\")\nos.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n</code></pre> <pre><code>llm = AzureChatOpenAI(deployment_name=\"chatgpt-gpt35-turbo\",model_name=\"gpt-35-turbo\",temperature=0.0)\n</code></pre> <pre><code>prompt = ChatPromptTemplate.from_template(\n    \"What is the best name to describe \\\n    a company that makes {product}?\"\n)\n</code></pre> <pre><code>chain = LLMChain(llm=llm, prompt=prompt)\n</code></pre> <pre><code>product = \"Queen Size Sheet Set\"\nchain.run(product)\n</code></pre> <pre><code>'Royal Linens.'\n</code></pre>"},{"location":"Chains/#simplesequentialchain","title":"SimpleSequentialChain","text":"<pre><code>from langchain.chains import SimpleSequentialChain\n</code></pre> <pre><code># prompt template 1\nfirst_prompt = ChatPromptTemplate.from_template(\n    \"What is the best name to describe \\\n    a company that makes {product}?\"\n)\n\n# Chain 1\nchain_one = LLMChain(llm=llm, prompt=first_prompt)\n</code></pre> <pre><code># prompt template 2\nsecond_prompt = ChatPromptTemplate.from_template(\n    \"Write a 20 words description for the following \\\n    company:{company_name}\"\n)\n# chain 2\nchain_two = LLMChain(llm=llm, prompt=second_prompt)\n</code></pre> <pre><code>overall_simple_chain = SimpleSequentialChain(chains=[chain_one, chain_two],\n                                             verbose=True\n                                            )\n</code></pre> <pre><code>overall_simple_chain.run(product)\n</code></pre> <pre><code>Error in on_chain_start callback: 'name'\n\n\n\u001b[36;1m\u001b[1;3mRoyal Linens.\u001b[0m\n\u001b[33;1m\u001b[1;3mRoyal Linens is a luxury bedding company that offers high-quality linens and bedding accessories for a comfortable and stylish home.\u001b[0m\n\n\u001b[1m&gt; Finished chain.\u001b[0m\n\n\n\n\n\n'Royal Linens is a luxury bedding company that offers high-quality linens and bedding accessories for a comfortable and stylish home.'\n</code></pre>"},{"location":"Chains/#sequentialchain","title":"SequentialChain","text":"<pre><code>from langchain.chains import SequentialChain\n</code></pre> <pre><code># prompt template 1: translate to english\nfirst_prompt = ChatPromptTemplate.from_template(\n    \"Translate the following review to english:\"\n    \"\\n\\n{Review}\"\n)\n# chain 1: input= Review and output= English_Review\nchain_one = LLMChain(llm=llm, prompt=first_prompt, \n                     output_key=\"English_Review\"\n                    )\n</code></pre> <pre><code>second_prompt = ChatPromptTemplate.from_template(\n    \"Can you summarize the following review in 1 sentence:\"\n    \"\\n\\n{English_Review}\"\n)\n# chain 2: input= English_Review and output= summary\nchain_two = LLMChain(llm=llm, prompt=second_prompt, \n                     output_key=\"summary\"\n                    )\n</code></pre> <pre><code># prompt template 3: translate to english\nthird_prompt = ChatPromptTemplate.from_template(\n    \"What language is the following review:\\n\\n{Review}\"\n)\n# chain 3: input= Review and output= language\nchain_three = LLMChain(llm=llm, prompt=third_prompt,\n                       output_key=\"language\"\n                      )\n</code></pre> <pre><code># prompt template 4: follow up message\nfourth_prompt = ChatPromptTemplate.from_template(\n    \"Write a follow up response to the following \"\n    \"summary in the specified language:\"\n    \"\\n\\nSummary: {summary}\\n\\nLanguage: {language}\"\n)\n# chain 4: input= summary, language and output= followup_message\nchain_four = LLMChain(llm=llm, prompt=fourth_prompt,\n                      output_key=\"followup_message\"\n                     )\n</code></pre> <pre><code># overall_chain: input= Review \n# and output= English_Review,summary, followup_message\noverall_chain = SequentialChain(\n    chains=[chain_one, chain_two, chain_three, chain_four],\n    input_variables=[\"Review\"],\n    output_variables=[\"English_Review\", \"summary\",\"followup_message\"],\n    verbose=True\n)\n</code></pre> <pre><code>review = df.Review[0]\noverall_chain(review)\n</code></pre> <pre><code>Error in on_chain_start callback: 'name'\n\n\n\n\u001b[1m&gt; Finished chain.\u001b[0m\n\n\n\n\n\n{'Review': 'I ordered a king size set. My only criticism would be that I wish seller would offer the king size set with 4 pillowcases. I separately ordered a two pack of pillowcases so I could have a total of four. When I saw the two packages, it looked like the color did not exactly match. Customer service was excellent about sending me two more pillowcases so I would have four that matched. Excellent! For the cost of these sheets, I am satisfied with the characteristics and coolness of the sheets.',\n 'English_Review': \"I ordered a king size bedding set, but my only complaint is that I wish the seller offered the set with four pillowcases instead of two. I had to order an additional two pack of pillowcases separately to have a total of four. When I received the two packages, it seemed like the color didn't match exactly. However, the customer service was great and they sent me two more pillowcases that matched perfectly. Overall, I am satisfied with the quality and coolness of the sheets considering their cost.\",\n 'summary': 'The reviewer was satisfied with the quality and coolness of the sheets, but wished the bedding set came with four pillowcases instead of two and had some issues with color matching, but customer service was great and sent matching pillowcases.',\n 'followup_message': \"Thank you for your review! We're glad to hear that you're satisfied with the quality and coolness of our sheets. We apologize for any inconvenience caused by the bedding set only coming with two pillowcases instead of four and the color matching issues. However, we're happy to hear that our customer service team was able to assist you and send matching pillowcases. We appreciate your feedback and hope to continue providing you with great products and service.\"}\n</code></pre>"},{"location":"Chains/#router-chain","title":"Router Chain","text":"<pre><code>physics_template = \"\"\"You are a very smart physics professor. \\\nYou are great at answering questions about physics in a concise\\\nand easy to understand manner. \\\nWhen you don't know the answer to a question you admit\\\nthat you don't know.\n\nHere is a question:\n{input}\"\"\"\n\n\nmath_template = \"\"\"You are a very good mathematician. \\\nYou are great at answering math questions. \\\nYou are so good because you are able to break down \\\nhard problems into their component parts, \nanswer the component parts, and then put them together\\\nto answer the broader question.\n\nHere is a question:\n{input}\"\"\"\n\nhistory_template = \"\"\"You are a very good historian. \\\nYou have an excellent knowledge of and understanding of people,\\\nevents and contexts from a range of historical periods. \\\nYou have the ability to think, reflect, debate, discuss and \\\nevaluate the past. You have a respect for historical evidence\\\nand the ability to make use of it to support your explanations \\\nand judgements.\n\nHere is a question:\n{input}\"\"\"\n\n\ncomputerscience_template = \"\"\" You are a successful computer scientist.\\\nYou have a passion for creativity, collaboration,\\\nforward-thinking, confidence, strong problem-solving capabilities,\\\nunderstanding of theories and algorithms, and excellent communication \\\nskills. You are great at answering coding questions. \\\nYou are so good because you know how to solve a problem by \\\ndescribing the solution in imperative steps \\\nthat a machine can easily interpret and you know how to \\\nchoose a solution that has a good balance between \\\ntime complexity and space complexity. \n\nHere is a question:\n{input}\"\"\"\n</code></pre> <pre><code>prompt_infos = [\n    {\n        \"name\": \"physics\", \n        \"description\": \"Good for answering questions about physics\", \n        \"prompt_template\": physics_template\n    },\n    {\n        \"name\": \"math\", \n        \"description\": \"Good for answering math questions\", \n        \"prompt_template\": math_template\n    },\n    {\n        \"name\": \"History\", \n        \"description\": \"Good for answering history questions\", \n        \"prompt_template\": history_template\n    },\n    {\n        \"name\": \"computer science\", \n        \"description\": \"Good for answering computer science questions\", \n        \"prompt_template\": computerscience_template\n    },\n    {\n        \"name\": \"biology\", \n        \"description\": \"Good for answering biology questions\", \n        \"prompt_template\": computerscience_template\n    }\n]\n</code></pre> <pre><code>from langchain.chains.router import MultiPromptChain\nfrom langchain.chains.router.llm_router import LLMRouterChain,RouterOutputParser\nfrom langchain.prompts import PromptTemplate\n</code></pre> <pre><code>destination_chains = {}\nfor p_info in prompt_infos:\n    name = p_info[\"name\"]\n    prompt_template = p_info[\"prompt_template\"]\n    prompt = ChatPromptTemplate.from_template(template=prompt_template)\n    chain = LLMChain(llm=llm, prompt=prompt)\n    destination_chains[name] = chain  \n\ndestinations = [f\"{p['name']}: {p['description']}\" for p in prompt_infos]\ndestinations_str = \"\\n\".join(destinations)\n</code></pre> <pre><code>default_prompt = ChatPromptTemplate.from_template(\"{input}\")\ndefault_chain = LLMChain(llm=llm, prompt=default_prompt)\n</code></pre> <pre><code>MULTI_PROMPT_ROUTER_TEMPLATE = \"\"\"Given a raw text input to a \\\nlanguage model select the model prompt best suited for the input. \\\nYou will be given the names of the available prompts and a \\\ndescription of what the prompt is best suited for. \\\nYou may also revise the original input if you think that revising\\\nit will ultimately lead to a better response from the language model.\n\n&lt;&lt; FORMATTING &gt;&gt;\nReturn a markdown code snippet with a JSON object formatted to look like:\n```json\n{{{{\n    \"destination\": string \\ name of the prompt to use or \"DEFAULT\"\n    \"next_inputs\": string \\ a potentially modified version of the original input\n}}}}\n</code></pre> <p>REMEMBER: \"destination\" MUST be one of the candidate prompt \\ names specified below OR it can be \"DEFAULT\" if the input is not\\ well suited for any of the candidate prompts. REMEMBER: \"next_inputs\" can just be the original input \\ if you don't think any modifications are needed.</p> <p>&lt;&lt; CANDIDATE PROMPTS &gt;&gt;</p> <p>&lt;&lt; INPUT &gt;&gt; {{input}}</p> <p>&lt;&lt; OUTPUT (remember to include the ```json)&gt;&gt;\"\"\" <pre><code>```python\nrouter_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(\n    destinations=destinations_str\n)\nrouter_prompt = PromptTemplate(\n    template=router_template,\n    input_variables=[\"input\"],\n    output_parser=RouterOutputParser(),\n)\n\nrouter_chain = LLMRouterChain.from_llm(llm, router_prompt)\n</code></pre></p> <pre><code>chain = MultiPromptChain(router_chain=router_chain, \n                         destination_chains=destination_chains, \n                         default_chain=default_chain, verbose=True\n                        )\n</code></pre> <pre><code>chain.run(\"What is black body radiation?\")\n</code></pre> <pre><code>Error in on_chain_start callback: 'name'\n\n\nphysics: {'input': 'What is black body radiation?'}\n\u001b[1m&gt; Finished chain.\u001b[0m\n\n\n\n\n\n\"Black body radiation refers to the electromagnetic radiation emitted by a perfect black body, which is an object that absorbs all radiation that falls on it and emits radiation at all wavelengths. The radiation emitted by a black body depends only on its temperature and follows a specific distribution known as Planck's law. This type of radiation is important in understanding the behavior of stars, as well as in the development of technologies such as incandescent light bulbs and infrared cameras.\"\n</code></pre> <pre><code>chain.run(\"what is 2 + 2\")\n</code></pre> <pre><code>Error in on_chain_start callback: 'name'\n\n\nmath: {'input': 'what is 2 + 2'}\n\u001b[1m&gt; Finished chain.\u001b[0m\n\n\n\n\n\n'As an AI language model, I can answer this question easily. The answer to 2 + 2 is 4.'\n</code></pre> <pre><code>chain.run(\"Why does every cell in our body contain DNA?\")\n</code></pre> <pre><code>Error in on_chain_start callback: 'name'\n\n\nbiology: {'input': 'Why is DNA present in every cell of the human body?'}\n\u001b[1m&gt; Finished chain.\u001b[0m\n\n\n\n\n\n'DNA is present in every cell of the human body because it contains the genetic information that determines the characteristics and functions of each cell. This information is necessary for the proper development, growth, and maintenance of the body. Additionally, DNA replication ensures that each new cell produced by the body contains a complete and accurate copy of the genetic information.'\n</code></pre>"},{"location":"Evaluation/","title":"LangChain: Evaluation","text":""},{"location":"Evaluation/#outline","title":"Outline:","text":"<ul> <li>Example generation</li> <li>Manual evaluation (and debuging)</li> <li>LLM-assisted evaluation</li> </ul> <pre><code>import os\n\nfrom dotenv import load_dotenv, find_dotenv\n_ = load_dotenv(find_dotenv()) # read local .env file\n</code></pre> <pre><code># Set OpenAI API key\nos.environ[\"OPENAI_API_TYPE\"] = os.getenv(\"api_type\")\nos.environ[\"OPENAI_API_BASE\"] = os.getenv(\"api_base\")\nos.environ[\"OPENAI_API_VERSION\"] = os.getenv(\"api_version\")\nos.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n</code></pre>"},{"location":"Evaluation/#create-our-qanda-application","title":"Create our QandA application","text":"<pre><code>from langchain.chains import RetrievalQA\nfrom langchain.chat_models import AzureChatOpenAI\nfrom langchain.document_loaders import CSVLoader\nfrom langchain.indexes import VectorstoreIndexCreator\nfrom langchain.vectorstores import DocArrayInMemorySearch\nfrom langchain.embeddings import OpenAIEmbeddings\n</code></pre> <pre><code>file = 'OutdoorClothingCatalog_1000.csv'\nloader = CSVLoader(file_path=file, encoding='utf-8')\ndata = loader.load()\n</code></pre> <pre><code>embeddings = OpenAIEmbeddings(model = \"text-embedding-ada-002\",chunk_size=1)\n</code></pre> <pre><code>index = VectorstoreIndexCreator(\n    vectorstore_cls=DocArrayInMemorySearch,\n    embedding = embeddings\n).from_loaders([loader])\n</code></pre> <pre><code>llm = AzureChatOpenAI(deployment_name=\"chatgpt-gpt35-turbo\",model_name=\"gpt-35-turbo\",temperature=0.0)\nqa = RetrievalQA.from_chain_type(\n    llm=llm, \n    chain_type=\"stuff\", \n    retriever=index.vectorstore.as_retriever(), \n    verbose=True,\n    chain_type_kwargs = {\n        \"document_separator\": \"&lt;&lt;&lt;&lt;&gt;&gt;&gt;&gt;&gt;\"\n    }\n)\n</code></pre>"},{"location":"Evaluation/#coming-up-with-test-datapoints","title":"Coming up with test datapoints","text":"<pre><code>data[2]\n</code></pre> <pre><code>Document(lc_kwargs={'page_content': \": 2\\nname: Infant and Toddler Girls' Coastal Chill Swimsuit, Two-Piece\\ndescription: She'll love the bright colors, ruffles and exclusive whimsical prints of this toddler's two-piece swimsuit! Our four-way-stretch and chlorine-resistant fabric keeps its shape and resists snags. The UPF 50+ rated fabric provides the highest rated sun protection possible, blocking 98% of the sun's harmful rays. The crossover no-slip straps and fully lined bottom ensure a secure fit and maximum coverage. Machine wash and line dry for best results. Imported.\", 'metadata': {'source': 'OutdoorClothingCatalog_1000.csv', 'row': 2}}, page_content=\": 2\\nname: Infant and Toddler Girls' Coastal Chill Swimsuit, Two-Piece\\ndescription: She'll love the bright colors, ruffles and exclusive whimsical prints of this toddler's two-piece swimsuit! Our four-way-stretch and chlorine-resistant fabric keeps its shape and resists snags. The UPF 50+ rated fabric provides the highest rated sun protection possible, blocking 98% of the sun's harmful rays. The crossover no-slip straps and fully lined bottom ensure a secure fit and maximum coverage. Machine wash and line dry for best results. Imported.\", metadata={'source': 'OutdoorClothingCatalog_1000.csv', 'row': 2})\n</code></pre> <pre><code>data[3]\n</code></pre> <pre><code>Document(lc_kwargs={'page_content': \": 3\\nname: Refresh Swimwear, V-Neck Tankini Contrasts\\ndescription: Whether you're going for a swim or heading out on an SUP, this watersport-ready tankini top is designed to move with you and stay comfortable. All while looking great in an eye-catching colorblock style. \\n\\nSize &amp; Fit\\nFitted: Sits close to the body.\\n\\nWhy We Love It\\nNot only does this swimtop feel good to wear, its fabric is good for the earth too. In recycled nylon, with Lycra\u00ae spandex for the perfect amount of stretch. \\n\\nFabric &amp; Care\\nThe premium Italian-blend is breathable, quick drying and abrasion resistant. \\nBody in 82% recycled nylon with 18% Lycra\u00ae spandex. \\nLined in 90% recycled nylon with 10% Lycra\u00ae spandex. \\nUPF 50+ rated \u2013 the highest rated sun protection possible. \\nHandwash, line dry.\\n\\nAdditional Features\\nLightweight racerback straps are easy to get on and off, and won't get in your way. \\nFlattering V-neck silhouette. \\nImported.\\n\\nSun Protection That Won't Wear Off\\nOur high-performance fabric provides SPF\", 'metadata': {'source': 'OutdoorClothingCatalog_1000.csv', 'row': 3}}, page_content=\": 3\\nname: Refresh Swimwear, V-Neck Tankini Contrasts\\ndescription: Whether you're going for a swim or heading out on an SUP, this watersport-ready tankini top is designed to move with you and stay comfortable. All while looking great in an eye-catching colorblock style. \\n\\nSize &amp; Fit\\nFitted: Sits close to the body.\\n\\nWhy We Love It\\nNot only does this swimtop feel good to wear, its fabric is good for the earth too. In recycled nylon, with Lycra\u00ae spandex for the perfect amount of stretch. \\n\\nFabric &amp; Care\\nThe premium Italian-blend is breathable, quick drying and abrasion resistant. \\nBody in 82% recycled nylon with 18% Lycra\u00ae spandex. \\nLined in 90% recycled nylon with 10% Lycra\u00ae spandex. \\nUPF 50+ rated \u2013 the highest rated sun protection possible. \\nHandwash, line dry.\\n\\nAdditional Features\\nLightweight racerback straps are easy to get on and off, and won't get in your way. \\nFlattering V-neck silhouette. \\nImported.\\n\\nSun Protection That Won't Wear Off\\nOur high-performance fabric provides SPF\", metadata={'source': 'OutdoorClothingCatalog_1000.csv', 'row': 3})\n</code></pre>"},{"location":"Evaluation/#hard-coded-examples","title":"Hard-coded examples","text":"<pre><code>examples = [\n    {\n        \"query\": \"Do the Cozy Comfort Pullover Set\\\n        have side pockets?\",\n        \"answer\": \"Yes\"\n    },\n    {\n        \"query\": \"What collection is the Ultra-Lofty \\\n        850 Stretch Down Hooded Jacket from?\",\n        \"answer\": \"The DownTek collection\"\n    }\n]\n</code></pre>"},{"location":"Evaluation/#llm-generated-examples","title":"LLM-Generated examples","text":"<pre><code>from langchain.evaluation.qa import QAGenerateChain\n</code></pre> <pre><code>example_gen_chain = QAGenerateChain.from_llm(llm)\n</code></pre> <pre><code>new_examples = example_gen_chain.apply_and_parse(\n    [{\"doc\": t} for t in data[:5]]\n)\n</code></pre> <pre><code>new_examples[0]\n</code></pre> <pre><code>{'query': \"What is the weight of the Women's Campside Oxfords per pair?\",\n 'answer': \"The Women's Campside Oxfords weigh approximately 1 lb. 1 oz. per pair.\"}\n</code></pre> <pre><code>data[0]\n</code></pre> <pre><code>Document(lc_kwargs={'page_content': \": 0\\nname: Women's Campside Oxfords\\ndescription: This ultracomfortable lace-to-toe Oxford boasts a super-soft canvas, thick cushioning, and quality construction for a broken-in feel from the first time you put them on. \\n\\nSize &amp; Fit: Order regular shoe size. For half sizes not offered, order up to next whole size. \\n\\nSpecs: Approx. weight: 1 lb.1 oz. per pair. \\n\\nConstruction: Soft canvas material for a broken-in feel and look. Comfortable EVA innersole with Cleansport NXT\u00ae antimicrobial odor control. Vintage hunt, fish and camping motif on innersole. Moderate arch contour of innersole. EVA foam midsole for cushioning and support. Chain-tread-inspired molded rubber outsole with modified chain-tread pattern. Imported. \\n\\nQuestions? Please contact us for any inquiries.\", 'metadata': {'source': 'OutdoorClothingCatalog_1000.csv', 'row': 0}}, page_content=\": 0\\nname: Women's Campside Oxfords\\ndescription: This ultracomfortable lace-to-toe Oxford boasts a super-soft canvas, thick cushioning, and quality construction for a broken-in feel from the first time you put them on. \\n\\nSize &amp; Fit: Order regular shoe size. For half sizes not offered, order up to next whole size. \\n\\nSpecs: Approx. weight: 1 lb.1 oz. per pair. \\n\\nConstruction: Soft canvas material for a broken-in feel and look. Comfortable EVA innersole with Cleansport NXT\u00ae antimicrobial odor control. Vintage hunt, fish and camping motif on innersole. Moderate arch contour of innersole. EVA foam midsole for cushioning and support. Chain-tread-inspired molded rubber outsole with modified chain-tread pattern. Imported. \\n\\nQuestions? Please contact us for any inquiries.\", metadata={'source': 'OutdoorClothingCatalog_1000.csv', 'row': 0})\n</code></pre>"},{"location":"Evaluation/#combine-examples","title":"Combine examples","text":"<pre><code>examples += new_examples\n</code></pre> <pre><code>qa.run(examples[0][\"query\"])\n</code></pre> <pre><code>Error in on_chain_start callback: 'name'\n\n\n\n\u001b[1m&gt; Finished chain.\u001b[0m\n\n\n\n\n\n\"I'm sorry, but I don't have any information about the Cozy Comfort Pullover Set. The context provided does not mention this product.\"\n</code></pre>"},{"location":"Evaluation/#manual-evaluation","title":"Manual Evaluation","text":"<pre><code>import langchain\nlangchain.debug = True\n</code></pre> <pre><code>qa.run(examples[0][\"query\"])\n</code></pre> <pre><code>\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RetrievalQA] Entering Chain run with input:\n\u001b[0m{\n  \"query\": \"Do the Cozy Comfort Pullover Set        have side pockets?\"\n}\n\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RetrievalQA &gt; 2:chain:StuffDocumentsChain] Entering Chain run with input:\n\u001b[0m[inputs]\n\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RetrievalQA &gt; 2:chain:StuffDocumentsChain &gt; 3:chain:LLMChain] Entering Chain run with input:\n\u001b[0m{\n  \"question\": \"Do the Cozy Comfort Pullover Set        have side pockets?\",\n  \"context\": \"Additional Features: Three-layer shell delivers waterproof protection. Brand new TEK O2 technology provides enhanced breathability. Interior gaiters keep out rain and snow. Full side zips for easy on/off over boots. Two zippered hand pockets. Thigh pocket. Imported.\\n\\n \u2013 Official Supplier to the U.S. Ski Team\\nTHEIR WILL&lt;&lt;&lt;&lt;&gt;&gt;&gt;&gt;&gt;: 5\\nname: Smooth Comfort Check Shirt, Slightly Fitted\\ndescription: Our men's slightly fitted check shirt is the perfect choice for your wardrobe! Customers love how it fits right out of the dryer. Size &amp; Fit: Slightly Fitted, Relaxed through the chest and sleeve with a slightly slimmer waist. Fabric &amp; Care: 100% cotton poplin, with wrinkle-free performance that won't wash out. Our innovative TrueCool\u00ae fabric wicks moisture away from your skin and helps it dry quickly. Additional Features: Traditional styling with a button-down collar and a single patch pocket. Imported.&lt;&lt;&lt;&lt;&gt;&gt;&gt;&gt;&gt;: 4\\nname: EcoFlex 3L Storm Pants\\ndescription: Our new TEK O2 technology makes our four-season waterproof pants even more breathable. It's guaranteed to keep you dry and comfortable \u2013 whatever the activity and whatever the weather. Size &amp; Fit: Slightly Fitted through hip and thigh. \\n\\nWhy We Love It: Our state-of-the-art TEK O2 technology offers the most breathability we've ever tested. Great as ski pants, they're ideal for a variety of outdoor activities year-round. Plus, they're loaded with features outdoor enthusiasts appreciate, including weather-blocking gaiters and handy side zips. Air In. Water Out. See how our air-permeable TEK O2 technology keeps you dry and comfortable. \\n\\nFabric &amp; Care: 100% nylon, exclusive of trim. Machine wash and dry.&lt;&lt;&lt;&lt;&gt;&gt;&gt;&gt;&gt;: 2\\nname: Infant and Toddler Girls' Coastal Chill Swimsuit, Two-Piece\\ndescription: She'll love the bright colors, ruffles and exclusive whimsical prints of this toddler's two-piece swimsuit! Our four-way-stretch and chlorine-resistant fabric keeps its shape and resists snags. The UPF 50+ rated fabric provides the highest rated sun protection possible, blocking 98% of the sun's harmful rays. The crossover no-slip straps and fully lined bottom ensure a secure fit and maximum coverage. Machine wash and line dry for best results. Imported.\"\n}\n\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:RetrievalQA &gt; 2:chain:StuffDocumentsChain &gt; 3:chain:LLMChain &gt; 4:llm:AzureChatOpenAI] Entering LLM run with input:\n\u001b[0m{\n  \"prompts\": [\n    \"System: Use the following pieces of context to answer the users question. \\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\\n----------------\\nAdditional Features: Three-layer shell delivers waterproof protection. Brand new TEK O2 technology provides enhanced breathability. Interior gaiters keep out rain and snow. Full side zips for easy on/off over boots. Two zippered hand pockets. Thigh pocket. Imported.\\n\\n \u2013 Official Supplier to the U.S. Ski Team\\nTHEIR WILL&lt;&lt;&lt;&lt;&gt;&gt;&gt;&gt;&gt;: 5\\nname: Smooth Comfort Check Shirt, Slightly Fitted\\ndescription: Our men's slightly fitted check shirt is the perfect choice for your wardrobe! Customers love how it fits right out of the dryer. Size &amp; Fit: Slightly Fitted, Relaxed through the chest and sleeve with a slightly slimmer waist. Fabric &amp; Care: 100% cotton poplin, with wrinkle-free performance that won't wash out. Our innovative TrueCool\u00ae fabric wicks moisture away from your skin and helps it dry quickly. Additional Features: Traditional styling with a button-down collar and a single patch pocket. Imported.&lt;&lt;&lt;&lt;&gt;&gt;&gt;&gt;&gt;: 4\\nname: EcoFlex 3L Storm Pants\\ndescription: Our new TEK O2 technology makes our four-season waterproof pants even more breathable. It's guaranteed to keep you dry and comfortable \u2013 whatever the activity and whatever the weather. Size &amp; Fit: Slightly Fitted through hip and thigh. \\n\\nWhy We Love It: Our state-of-the-art TEK O2 technology offers the most breathability we've ever tested. Great as ski pants, they're ideal for a variety of outdoor activities year-round. Plus, they're loaded with features outdoor enthusiasts appreciate, including weather-blocking gaiters and handy side zips. Air In. Water Out. See how our air-permeable TEK O2 technology keeps you dry and comfortable. \\n\\nFabric &amp; Care: 100% nylon, exclusive of trim. Machine wash and dry.&lt;&lt;&lt;&lt;&gt;&gt;&gt;&gt;&gt;: 2\\nname: Infant and Toddler Girls' Coastal Chill Swimsuit, Two-Piece\\ndescription: She'll love the bright colors, ruffles and exclusive whimsical prints of this toddler's two-piece swimsuit! Our four-way-stretch and chlorine-resistant fabric keeps its shape and resists snags. The UPF 50+ rated fabric provides the highest rated sun protection possible, blocking 98% of the sun's harmful rays. The crossover no-slip straps and fully lined bottom ensure a secure fit and maximum coverage. Machine wash and line dry for best results. Imported.\\nHuman: Do the Cozy Comfort Pullover Set        have side pockets?\"\n  ]\n}\n\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:RetrievalQA &gt; 2:chain:StuffDocumentsChain &gt; 3:chain:LLMChain &gt; 4:llm:AzureChatOpenAI] [1.13s] Exiting LLM run with output:\n\u001b[0m{\n  \"generations\": [\n    [\n      {\n        \"text\": \"I'm sorry, but I don't have any information about the Cozy Comfort Pullover Set. The context provided does not mention this product.\",\n        \"generation_info\": null,\n        \"message\": {\n          \"content\": \"I'm sorry, but I don't have any information about the Cozy Comfort Pullover Set. The context provided does not mention this product.\",\n          \"additional_kwargs\": {},\n          \"example\": false\n        }\n      }\n    ]\n  ],\n  \"llm_output\": {\n    \"token_usage\": {\n      \"completion_tokens\": 29,\n      \"prompt_tokens\": 561,\n      \"total_tokens\": 590\n    },\n    \"model_name\": \"gpt-35-turbo\"\n  },\n  \"run\": null\n}\n\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RetrievalQA &gt; 2:chain:StuffDocumentsChain &gt; 3:chain:LLMChain] [1.13s] Exiting Chain run with output:\n\u001b[0m{\n  \"text\": \"I'm sorry, but I don't have any information about the Cozy Comfort Pullover Set. The context provided does not mention this product.\"\n}\n\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RetrievalQA &gt; 2:chain:StuffDocumentsChain] [1.13s] Exiting Chain run with output:\n\u001b[0m{\n  \"output_text\": \"I'm sorry, but I don't have any information about the Cozy Comfort Pullover Set. The context provided does not mention this product.\"\n}\n\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RetrievalQA] [1.57s] Exiting Chain run with output:\n\u001b[0m{\n  \"result\": \"I'm sorry, but I don't have any information about the Cozy Comfort Pullover Set. The context provided does not mention this product.\"\n}\n\n\n\n\n\n\"I'm sorry, but I don't have any information about the Cozy Comfort Pullover Set. The context provided does not mention this product.\"\n</code></pre> <pre><code># Turn off the debug mode\nlangchain.debug = False\n</code></pre>"},{"location":"Evaluation/#llm-assisted-evaluation","title":"LLM assisted evaluation","text":"<pre><code>predictions = qa.apply(examples)\n</code></pre> <pre><code>Error in on_chain_start callback: 'name'\nError in on_chain_start callback: 'name'\n\n\n\n\u001b[1m&gt; Finished chain.\u001b[0m\n\n\nError in on_chain_start callback: 'name'\n\n\n\n\u001b[1m&gt; Finished chain.\u001b[0m\n\n\nError in on_chain_start callback: 'name'\n\n\n\n\u001b[1m&gt; Finished chain.\u001b[0m\n\n\nError in on_chain_start callback: 'name'\n\n\n\n\u001b[1m&gt; Finished chain.\u001b[0m\n\n\nError in on_chain_start callback: 'name'\n\n\n\n\u001b[1m&gt; Finished chain.\u001b[0m\n\n\nError in on_chain_start callback: 'name'\n\n\n\n\u001b[1m&gt; Finished chain.\u001b[0m\n\n\u001b[1m&gt; Finished chain.\u001b[0m\n</code></pre> <pre><code>from langchain.evaluation.qa import QAEvalChain\n</code></pre> <pre><code>eval_chain = QAEvalChain.from_llm(llm)\n</code></pre> <pre><code>graded_outputs = eval_chain.evaluate(examples, predictions)\n</code></pre> <pre><code>for i, eg in enumerate(examples):\n    print(f\"Example {i}:\")\n    print(\"Question: \" + predictions[i]['query'])\n    print(\"Real Answer: \" + predictions[i]['answer'])\n    print(\"Predicted Answer: \" + predictions[i]['result'])\n    print(\"Predicted Grade: \" + graded_outputs[i]['text'])\n    print()\n</code></pre> <pre><code>Example 0:\nQuestion: Do the Cozy Comfort Pullover Set        have side pockets?\nReal Answer: Yes\nPredicted Answer: I'm sorry, but I don't have any information about the Cozy Comfort Pullover Set. The context provided does not mention this product.\nPredicted Grade: INCORRECT\n\nExample 1:\nQuestion: What collection is the Ultra-Lofty         850 Stretch Down Hooded Jacket from?\nReal Answer: The DownTek collection\nPredicted Answer: There is no information provided about the Ultra-Lofty 850 Stretch Down Hooded Jacket's collection.\nPredicted Grade: INCORRECT\n\nExample 2:\nQuestion: What is the weight of the Women's Campside Oxfords per pair?\nReal Answer: The Women's Campside Oxfords weigh approximately 1 lb. 1 oz. per pair.\nPredicted Answer: The Women's Campside Oxfords weigh approximately 1 lb. 1 oz. per pair.\nPredicted Grade: CORRECT\n\nExample 3:\nQuestion: What are the dimensions of the small and medium Recycled Waterhog Dog Mat?\nReal Answer: The small Recycled Waterhog Dog Mat has dimensions of 18\" x 28\" and the medium has dimensions of 22.5\" x 34.5\".\nPredicted Answer: The small Recycled Waterhog Dog Mat has dimensions of 18\" x 28\" and the medium size has dimensions of 22.5\" x 34.5\".\nPredicted Grade: CORRECT\n\nExample 4:\nQuestion: What are the features of the Infant and Toddler Girls' Coastal Chill Swimsuit, Two-Piece?\nReal Answer: The Infant and Toddler Girls' Coastal Chill Swimsuit, Two-Piece features bright colors, ruffles, and exclusive whimsical prints. The four-way-stretch and chlorine-resistant fabric keeps its shape and resists snags. The UPF 50+ rated fabric provides the highest rated sun protection possible, blocking 98% of the sun's harmful rays. The crossover no-slip straps and fully lined bottom ensure a secure fit and maximum coverage. It can be machine washed and line dried for best results.\nPredicted Answer: The Infant and Toddler Girls' Coastal Chill Swimsuit, Two-Piece features bright colors, ruffles, and exclusive whimsical prints. The swimsuit is made of four-way-stretch and chlorine-resistant fabric that keeps its shape and resists snags. The UPF 50+ rated fabric provides the highest rated sun protection possible, blocking 98% of the sun's harmful rays. The crossover no-slip straps and fully lined bottom ensure a secure fit and maximum coverage. The swimsuit can be machine washed and line dried for best results.\nPredicted Grade: CORRECT\n\nExample 5:\nQuestion: What is the fabric composition of the Refresh Swimwear, V-Neck Tankini Contrasts?\nReal Answer: The Refresh Swimwear, V-Neck Tankini Contrasts is made of 82% recycled nylon and 18% Lycra\u00ae spandex for the body, and 90% recycled nylon and 10% Lycra\u00ae spandex for the lining.\nPredicted Answer: The Refresh Swimwear, V-Neck Tankini Contrasts is made of 82% recycled nylon with 18% Lycra\u00ae spandex for the body and 90% recycled nylon with 10% Lycra\u00ae spandex for the lining.\nPredicted Grade: CORRECT\n\nExample 6:\nQuestion: What is the new technology used in the EcoFlex 3L Storm Pants and what is its benefit?\nReal Answer: The new technology used in the EcoFlex 3L Storm Pants is TEK O2 technology, which makes the pants more breathable. Its benefit is that it keeps the wearer dry and comfortable in any activity and weather.\nPredicted Answer: The new technology used in the EcoFlex 3L Storm Pants is called TEK O2 technology, which offers enhanced breathability. This means that the pants are more comfortable to wear and will keep you dry and comfortable during outdoor activities, no matter the weather.\nPredicted Grade: CORRECT\n</code></pre> <pre><code>\n</code></pre>"},{"location":"Memory/","title":"LangChain: Memory","text":""},{"location":"Memory/#outline","title":"Outline","text":"<ul> <li>ConversationBufferMemory</li> <li>ConversationBufferWindowMemory</li> <li>ConversationTokenBufferMemory</li> <li>ConversationSummaryMemory</li> </ul>"},{"location":"Memory/#conversationbuffermemory","title":"ConversationBufferMemory","text":"<pre><code>## Get your OpenAI API\nimport openai\nimport os\nfrom langchain.chat_models import AzureChatOpenAI\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.chains import ConversationChain\nfrom langchain.memory import ConversationBufferMemory\n\nfrom dotenv import load_dotenv, find_dotenv\n_ = load_dotenv(find_dotenv())\n</code></pre> <pre><code># Set OpenAI API key\nos.environ[\"OPENAI_API_TYPE\"] = os.getenv(\"api_type\")\nos.environ[\"OPENAI_API_BASE\"] = os.getenv(\"api_base\")\nos.environ[\"OPENAI_API_VERSION\"] = os.getenv(\"api_version\")\nos.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n</code></pre> <pre><code>llm = AzureChatOpenAI(deployment_name=\"chatgpt-gpt35-turbo\",model_name=\"gpt-35-turbo\",temperature=0.0)\n</code></pre> <pre><code>memory = ConversationBufferMemory()\nconversation = ConversationChain(\n    llm=llm, \n    memory = memory,\n    verbose=True\n)\n</code></pre> <pre><code>conversation.predict(input=\"Hi, my name is Andrew\")\n</code></pre> <pre><code>Error in on_chain_start callback: 'name'\n\n\nPrompt after formatting:\n\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n\nCurrent conversation:\n\nHuman: Hi, my name is Andrew\nAI:\u001b[0m\n\n\u001b[1m&gt; Finished chain.\u001b[0m\n\n\n\n\n\n\"Hello Andrew, it's nice to meet you. My name is AI. How can I assist you today?\"\n</code></pre> <pre><code>conversation.predict(input=\"What is 1+1?\")\n</code></pre> <pre><code>Error in on_chain_start callback: 'name'\n\n\nPrompt after formatting:\n\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n\nCurrent conversation:\nHuman: Hi, my name is Andrew\nAI: Hello Andrew, it's nice to meet you. My name is AI. How can I assist you today?\nHuman: What is 1+1?\nAI:\u001b[0m\n\n\u001b[1m&gt; Finished chain.\u001b[0m\n\n\n\n\n\n'The answer to 1+1 is 2.'\n</code></pre> <pre><code>conversation.predict(input=\"What is my name?\")\n</code></pre> <pre><code>Error in on_chain_start callback: 'name'\n\n\nPrompt after formatting:\n\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n\nCurrent conversation:\nHuman: Hi, my name is Andrew\nAI: Hello Andrew, it's nice to meet you. My name is AI. How can I assist you today?\nHuman: What is 1+1?\nAI: The answer to 1+1 is 2.\nHuman: What is my name?\nAI:\u001b[0m\n\n\u001b[1m&gt; Finished chain.\u001b[0m\n\n\n\n\n\n'Your name is Andrew, as you mentioned earlier.'\n</code></pre> <pre><code>print(memory.buffer)\n</code></pre> <pre><code>Human: Hi, my name is Andrew\nAI: Hello Andrew, it's nice to meet you. My name is AI. How can I assist you today?\nHuman: What is 1+1?\nAI: The answer to 1+1 is 2.\nHuman: What is my name?\nAI: Your name is Andrew, as you mentioned earlier.\n</code></pre> <pre><code>memory.load_memory_variables({})\n</code></pre> <pre><code>{'history': \"Human: Hi, my name is Andrew\\nAI: Hello Andrew, it's nice to meet you. My name is AI. How can I assist you today?\\nHuman: What is 1+1?\\nAI: The answer to 1+1 is 2.\\nHuman: What is my name?\\nAI: Your name is Andrew, as you mentioned earlier.\"}\n</code></pre> <pre><code>memory = ConversationBufferMemory()\n</code></pre> <pre><code>memory.save_context({\"input\": \"Hi\"}, \n                    {\"output\": \"What's up\"})\n</code></pre> <pre><code>print(memory.buffer)\n</code></pre> <pre><code>Human: Hi\nAI: What's up\n</code></pre> <pre><code>memory.load_memory_variables({})\n</code></pre> <pre><code>{'history': \"Human: Hi\\nAI: What's up\"}\n</code></pre> <pre><code>memory.save_context({\"input\": \"Not much, just hanging\"}, \n                    {\"output\": \"Cool\"})\n</code></pre> <pre><code>memory.load_memory_variables({})\n</code></pre> <pre><code>{'history': \"Human: Hi\\nAI: What's up\\nHuman: Not much, just hanging\\nAI: Cool\"}\n</code></pre>"},{"location":"Memory/#conversationbufferwindowmemory","title":"ConversationBufferWindowMemory","text":"<pre><code>from langchain.memory import ConversationBufferWindowMemory\n</code></pre> <pre><code>memory = ConversationBufferWindowMemory(k=1)\n</code></pre> <pre><code>memory.save_context({\"input\": \"Hi\"},\n                    {\"output\": \"What's up\"})\nmemory.save_context({\"input\": \"Not much, just hanging\"},\n                    {\"output\": \"Cool\"})\n</code></pre> <pre><code>memory.load_memory_variables({})\n</code></pre> <pre><code>{'history': 'Human: Not much, just hanging\\nAI: Cool'}\n</code></pre> <pre><code>memory = ConversationBufferWindowMemory(k=1)\nconversation = ConversationChain(\n    llm=llm, \n    memory = memory,\n    verbose=False\n)\n</code></pre> <pre><code>conversation.predict(input=\"Hi, my name is Andrew\")\n</code></pre> <pre><code>\"Hello Andrew, it's nice to meet you. My name is AI. How can I assist you today?\"\n</code></pre> <pre><code>conversation.predict(input=\"What is 1+1?\")\n</code></pre> <pre><code>'The answer to 1+1 is 2.'\n</code></pre> <pre><code>conversation.predict(input=\"What is my name?\")\n</code></pre> <pre><code>\"I'm sorry, I don't have access to that information. Could you please tell me your name?\"\n</code></pre>"},{"location":"Memory/#conversationtokenbuffermemory","title":"ConversationTokenBufferMemory","text":"<pre><code>#!pip install tiktoken\n</code></pre> <p>Below notebook will be helpfull to understand how to use tiktoken library to generate tokens for your application.</p> <p>https://github.com/pinecone-io/examples/blob/master/generation/langchain/handbook/03-langchain-conversational-memory.ipynb</p> <pre><code>from langchain.memory import ConversationTokenBufferMemory\nfrom langchain.llms import OpenAI\nimport tiktoken\n</code></pre> <pre><code>llm = OpenAI(\n    temperature=0, \n    model_name='text-davinci-003'  # can be used with llms like 'gpt-3.5-turbo'\n)\n</code></pre> <pre><code>memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=30)\nmemory.save_context({\"input\": \"AI is what?!\"},\n                    {\"output\": \"Amazing!\"})\nmemory.save_context({\"input\": \"Backpropagation is what?\"},\n                    {\"output\": \"Beautiful!\"})\nmemory.save_context({\"input\": \"Chatbots are what?\"}, \n                    {\"output\": \"Charming!\"})\n# Ideally you should use ChatGPT, I am getting error ```Warning: model not found. Using cl100k_base encoding.```. So, I am using Instruct GPT.\n</code></pre> <pre><code>memory.load_memory_variables({})\n</code></pre> <pre><code>{'history': 'AI: Amazing!\\nHuman: Backpropagation is what?\\nAI: Beautiful!\\nHuman: Chatbots are what?\\nAI: Charming!'}\n</code></pre>"},{"location":"Memory/#conversationsummarymemory","title":"ConversationSummaryMemory","text":"<pre><code>from langchain.memory import ConversationSummaryBufferMemory\n</code></pre> <pre><code># create a long string\nschedule = \"There is a meeting at 8am with your product team. \\\nYou will need your powerpoint presentation prepared. \\\n9am-12pm have time to work on your LangChain \\\nproject which will go quickly because Langchain is such a powerful tool. \\\nAt Noon, lunch at the italian resturant with a customer who is driving \\\nfrom over an hour away to meet you to understand the latest in AI. \\\nBe sure to bring your laptop to show the latest LLM demo.\"\n\nmemory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=4000)\nmemory.save_context({\"input\": \"Hello\"}, {\"output\": \"What's up\"})\nmemory.save_context({\"input\": \"Not much, just hanging\"},\n                    {\"output\": \"Cool\"})\nmemory.save_context({\"input\": \"What is on the schedule today?\"}, \n                    {\"output\": f\"{schedule}\"})\n</code></pre> <pre><code>memory.load_memory_variables({})\n</code></pre> <pre><code>{'history': \"Human: Hello\\nAI: What's up\\nHuman: Not much, just hanging\\nAI: Cool\\nHuman: What is on the schedule today?\\nAI: There is a meeting at 8am with your product team. You will need your powerpoint presentation prepared. 9am-12pm have time to work on your LangChain project which will go quickly because Langchain is such a powerful tool. At Noon, lunch at the italian resturant with a customer who is driving from over an hour away to meet you to understand the latest in AI. Be sure to bring your laptop to show the latest LLM demo.\"}\n</code></pre> <pre><code>llm_turbo = AzureChatOpenAI(deployment_name=\"chatgpt-gpt35-turbo\",model_name=\"gpt-35-turbo\",temperature=0.0)\n</code></pre> <pre><code>conversation = ConversationChain(\n    llm=llm_turbo, \n    memory = memory,\n    verbose=True\n)\n</code></pre> <pre><code>conversation.predict(input=\"What would be a good demo to show?\")\n</code></pre> <pre><code>Error in on_chain_start callback: 'name'\n\n\nPrompt after formatting:\n\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n\nCurrent conversation:\nHuman: Hello\nAI: What's up\nHuman: Not much, just hanging\nAI: Cool\nHuman: What is on the schedule today?\nAI: There is a meeting at 8am with your product team. You will need your powerpoint presentation prepared. 9am-12pm have time to work on your LangChain project which will go quickly because Langchain is such a powerful tool. At Noon, lunch at the italian resturant with a customer who is driving from over an hour away to meet you to understand the latest in AI. Be sure to bring your laptop to show the latest LLM demo.\nHuman: What would be a good demo to show?\nAI:\u001b[0m\n\n\u001b[1m&gt; Finished chain.\u001b[0m\n\n\n\n\n\n\"Based on the customer's interests, I would recommend showing the LLM's natural language processing capabilities and how it can be used to analyze large amounts of data quickly and accurately. You could also demonstrate how the LLM can be integrated with other AI tools to create a more comprehensive solution.\"\n</code></pre> <pre><code>memory.load_memory_variables({})\n</code></pre> <pre><code>{'history': \"Human: Hello\\nAI: What's up\\nHuman: Not much, just hanging\\nAI: Cool\\nHuman: What is on the schedule today?\\nAI: There is a meeting at 8am with your product team. You will need your powerpoint presentation prepared. 9am-12pm have time to work on your LangChain project which will go quickly because Langchain is such a powerful tool. At Noon, lunch at the italian resturant with a customer who is driving from over an hour away to meet you to understand the latest in AI. Be sure to bring your laptop to show the latest LLM demo.\\nHuman: What would be a good demo to show?\\nAI: Based on the customer's interests, I would recommend showing the LLM's natural language processing capabilities and how it can be used to analyze large amounts of data quickly and accurately. You could also demonstrate how the LLM can be integrated with other AI tools to create a more comprehensive solution.\"}\n</code></pre> <pre><code>\n</code></pre>"},{"location":"Model_prompt_parser/","title":"LangChain: Models, Prompts and Output Parsers","text":""},{"location":"Model_prompt_parser/#outline","title":"Outline","text":"<ul> <li>Direct API calls to OpenAI</li> <li>API calls through LangChain:</li> <li>Prompts</li> <li>Models</li> <li>Output parsers</li> </ul> <pre><code>#!pip install python-dotenv\n#!pip install openai\n</code></pre> <pre><code>## Get your OpenAI API\nimport openai\nimport os\n\nfrom dotenv import load_dotenv, find_dotenv\n_ = load_dotenv(find_dotenv())\n\nopenai.api_type = os.getenv(\"api_type\")\nopenai.api_base = os.getenv(\"api_base\")\nopenai.api_version = os.getenv(\"api_version\")\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")\n</code></pre>"},{"location":"Model_prompt_parser/#chat-api-openai","title":"Chat API : OpenAI","text":"<p>Let's start with a direct API calls to OpenAI.</p> <pre><code>def get_completion(prompt, model=\"chatgpt-gpt35-turbo\"):\n    messages = [{\"role\": \"user\", \"content\": prompt}]\n    response = openai.ChatCompletion.create(\n        engine=model,\n        messages=messages,\n        temperature=0, # this is the degree of randomness of the model's output\n        max_tokens=800,\n        top_p=0.95,\n        frequency_penalty=0,\n        presence_penalty=0,\n        stop=None)\n    return response.choices[0].message[\"content\"]\n</code></pre> <pre><code>get_completion(\"What is 1+1?\")\n</code></pre> <pre><code>'As an AI language model, I can tell you that the answer to 1+1 is 2.'\n</code></pre> <pre><code>customer_email = \"\"\"\nArrr, I be fuming that me blender lid \\\nflew off and splattered me kitchen walls \\\nwith smoothie! And to make matters worse,\\\nthe warranty don't cover the cost of \\\ncleaning up me kitchen. I need yer help \\\nright now, matey!\n\"\"\"\n</code></pre> <pre><code>style = \"\"\"American English \\\nin a calm and respectful tone\n\"\"\"\n</code></pre> <pre><code>prompt = f\"\"\"Translate the text \\\nthat is delimited by triple backticks \ninto a style that is {style}.\ntext: ```{customer_email}```\n\"\"\"\n\nprint(prompt)\n</code></pre> <pre><code>Translate the text that is delimited by triple backticks \ninto a style that is American English in a calm and respectful tone\n.\ntext: ```\nArrr, I be fuming that me blender lid flew off and splattered me kitchen walls with smoothie! And to make matters worse,the warranty don't cover the cost of cleaning up me kitchen. I need yer help right now, matey!\n```\n</code></pre> <p>```python response = get_completion(prompt) <pre><code>```python\nresponse\n</code></pre></p> <pre><code>'I am quite upset that my blender lid came off and caused my smoothie to splatter all over my kitchen walls. Additionally, the warranty does not cover the cost of cleaning up the mess. Would you be able to assist me, please? Thank you kindly.'\n</code></pre>"},{"location":"Model_prompt_parser/#chat-api-langchain","title":"Chat API : LangChain","text":"<p>Let's try how we can do the same using LangChain.</p> <pre><code>#!pip install --upgrade langchain\n</code></pre>"},{"location":"Model_prompt_parser/#model","title":"Model","text":"<pre><code>from langchain.chat_models import AzureChatOpenAI\n</code></pre> <pre><code># Set OpenAI API key\nos.environ[\"OPENAI_API_TYPE\"] = os.getenv(\"api_type\")\nos.environ[\"OPENAI_API_BASE\"] = os.getenv(\"api_base\")\nos.environ[\"OPENAI_API_VERSION\"] = os.getenv(\"api_version\")\nos.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n</code></pre> <pre><code># To control the randomness and creativity of the generated\n# text by an LLM, use temperature = 0.0\nchat = AzureChatOpenAI(deployment_name=\"chatgpt-gpt35-turbo\",model_name=\"gpt-35-turbo\",temperature=0.0)\n</code></pre>"},{"location":"Model_prompt_parser/#prompt-template","title":"Prompt template","text":"<pre><code>template_string = \"\"\"Translate the text \\\nthat is delimited by triple backticks \\\ninto a style that is {style}. \\\ntext: ```{text}```\n\"\"\"\n</code></pre> <pre><code>from langchain.prompts import ChatPromptTemplate\n\nprompt_template = ChatPromptTemplate.from_template(template_string)\n</code></pre> <pre><code>prompt_template.messages[0].prompt\n</code></pre> <pre><code>PromptTemplate(lc_kwargs={'input_variables': ['style', 'text'], 'template': 'Translate the text that is delimited by triple backticks into a style that is {style}. text: ```{text}```\\n'}, input_variables=['style', 'text'], output_parser=None, partial_variables={}, template='Translate the text that is delimited by triple backticks into a style that is {style}. text: ```{text}```\\n', template_format='f-string', validate_template=True)\n</code></pre> <pre><code>prompt_template.messages[0].prompt.input_variables\n</code></pre> <pre><code>['style', 'text']\n</code></pre> <pre><code>customer_style = \"\"\"American English \\\nin a calm and respectful tone\n\"\"\"\n</code></pre> <pre><code>customer_email = \"\"\"\nArrr, I be fuming that me blender lid \\\nflew off and splattered me kitchen walls \\\nwith smoothie! And to make matters worse, \\\nthe warranty don't cover the cost of \\\ncleaning up me kitchen. I need yer help \\\nright now, matey!\n\"\"\"\n</code></pre> <pre><code>customer_messages = prompt_template.format_messages(\n                    style=customer_style,\n                    text=customer_email)\n</code></pre> <pre><code>print(type(customer_messages))\nprint(type(customer_messages[0]))\n</code></pre> <pre><code>&lt;class 'list'&gt;\n&lt;class 'langchain.schema.HumanMessage'&gt;\n</code></pre> <pre><code>print(customer_messages[0])\n</code></pre> <pre><code>lc_kwargs={'content': \"Translate the text that is delimited by triple backticks into a style that is American English in a calm and respectful tone\\n. text: ```\\nArrr, I be fuming that me blender lid flew off and splattered me kitchen walls with smoothie! And to make matters worse, the warranty don't cover the cost of cleaning up me kitchen. I need yer help right now, matey!\\n```\\n\", 'additional_kwargs': {}} content=\"Translate the text that is delimited by triple backticks into a style that is American English in a calm and respectful tone\\n. text: ```\\nArrr, I be fuming that me blender lid flew off and splattered me kitchen walls with smoothie! And to make matters worse, the warranty don't cover the cost of cleaning up me kitchen. I need yer help right now, matey!\\n```\\n\" additional_kwargs={} example=False\n</code></pre> <pre><code># Call the LLM to translate to the style of the customer message\ncustomer_response = chat(customer_messages)\n</code></pre> <pre><code>print(customer_response.content)\n</code></pre> <pre><code>I'm really frustrated that my blender lid flew off and made a mess of my kitchen walls with smoothie. To add to my frustration, the warranty doesn't cover the cost of cleaning up my kitchen. Can you please help me out, friend?\n</code></pre> <pre><code>service_reply = \"\"\"Hey there customer, \\\nthe warranty does not cover \\\ncleaning expenses for your kitchen \\\nbecause it's your fault that \\\nyou misused your blender \\\nby forgetting to put the lid on before \\\nstarting the blender. \\\nTough luck! See ya!\n\"\"\"\n</code></pre> <pre><code>service_style_pirate = \"\"\"\\\na polite tone \\\nthat speaks in English Pirate\\\n\"\"\"\n</code></pre> <pre><code>service_messages = prompt_template.format_messages(\n    style=service_style_pirate,\n    text=service_reply)\n\nprint(service_messages[0].content)\n</code></pre> <pre><code>Translate the text that is delimited by triple backticks into a style that is a polite tone that speaks in English Pirate. text: ```Hey there customer, the warranty does not cover cleaning expenses for your kitchen because it's your fault that you misused your blender by forgetting to put the lid on before starting the blender. Tough luck! See ya!\n```\n</code></pre> <p>```python service_response = chat(service_messages) print(service_response.content) <pre><code>    Ahoy there, matey! I must kindly inform ye that the warranty be not coverin' the expenses o' cleaning yer galley, as 'tis yer own fault fer misusin' yer blender by forgettin' to put the lid on afore startin' it. Aye, tough luck! Farewell and may the winds be in yer favor!\n\n\n### Output Parsers\nLet's start with defining how we would like the LLM output to look like:\n\n\n```python\n{\n  \"gift\": False,\n  \"delivery_days\": 5,\n  \"price_value\": \"pretty affordable!\"\n}\n</code></pre></p> <pre><code>{'gift': False, 'delivery_days': 5, 'price_value': 'pretty affordable!'}\n</code></pre> <pre><code>customer_review = \"\"\"\\\nThis leaf blower is pretty amazing.  It has four settings:\\\ncandle blower, gentle breeze, windy city, and tornado. \\\nIt arrived in two days, just in time for my wife's \\\nanniversary present. \\\nI think my wife liked it so much she was speechless. \\\nSo far I've been the only one using it, and I've been \\\nusing it every other morning to clear the leaves on our lawn. \\\nIt's slightly more expensive than the other leaf blowers \\\nout there, but I think it's worth it for the extra features.\n\"\"\"\n\nreview_template = \"\"\"\\\nFor the following text, extract the following information:\n\ngift: Was the item purchased as a gift for someone else? \\\nAnswer True if yes, False if not or unknown.\n\ndelivery_days: How many days did it take for the product \\\nto arrive? If this information is not found, output -1.\n\nprice_value: Extract any sentences about the value or price,\\\nand output them as a comma separated Python list.\n\nFormat the output as JSON with the following keys:\ngift\ndelivery_days\nprice_value\n\ntext: {text}\n\"\"\"\n</code></pre> <pre><code>from langchain.prompts import ChatPromptTemplate\n\nprompt_template = ChatPromptTemplate.from_template(review_template)\nprint(prompt_template)\n</code></pre> <pre><code>lc_kwargs={'input_variables': ['text'], 'messages': [HumanMessagePromptTemplate(lc_kwargs={'prompt': PromptTemplate(lc_kwargs={'input_variables': ['text'], 'template': 'For the following text, extract the following information:\\n\\ngift: Was the item purchased as a gift for someone else? Answer True if yes, False if not or unknown.\\n\\ndelivery_days: How many days did it take for the product to arrive? If this information is not found, output -1.\\n\\nprice_value: Extract any sentences about the value or price,and output them as a comma separated Python list.\\n\\nFormat the output as JSON with the following keys:\\ngift\\ndelivery_days\\nprice_value\\n\\ntext: {text}\\n'}, input_variables=['text'], output_parser=None, partial_variables={}, template='For the following text, extract the following information:\\n\\ngift: Was the item purchased as a gift for someone else? Answer True if yes, False if not or unknown.\\n\\ndelivery_days: How many days did it take for the product to arrive? If this information is not found, output -1.\\n\\nprice_value: Extract any sentences about the value or price,and output them as a comma separated Python list.\\n\\nFormat the output as JSON with the following keys:\\ngift\\ndelivery_days\\nprice_value\\n\\ntext: {text}\\n', template_format='f-string', validate_template=True)}, prompt=PromptTemplate(lc_kwargs={'input_variables': ['text'], 'template': 'For the following text, extract the following information:\\n\\ngift: Was the item purchased as a gift for someone else? Answer True if yes, False if not or unknown.\\n\\ndelivery_days: How many days did it take for the product to arrive? If this information is not found, output -1.\\n\\nprice_value: Extract any sentences about the value or price,and output them as a comma separated Python list.\\n\\nFormat the output as JSON with the following keys:\\ngift\\ndelivery_days\\nprice_value\\n\\ntext: {text}\\n'}, input_variables=['text'], output_parser=None, partial_variables={}, template='For the following text, extract the following information:\\n\\ngift: Was the item purchased as a gift for someone else? Answer True if yes, False if not or unknown.\\n\\ndelivery_days: How many days did it take for the product to arrive? If this information is not found, output -1.\\n\\nprice_value: Extract any sentences about the value or price,and output them as a comma separated Python list.\\n\\nFormat the output as JSON with the following keys:\\ngift\\ndelivery_days\\nprice_value\\n\\ntext: {text}\\n', template_format='f-string', validate_template=True), additional_kwargs={})]} input_variables=['text'] output_parser=None partial_variables={} messages=[HumanMessagePromptTemplate(lc_kwargs={'prompt': PromptTemplate(lc_kwargs={'input_variables': ['text'], 'template': 'For the following text, extract the following information:\\n\\ngift: Was the item purchased as a gift for someone else? Answer True if yes, False if not or unknown.\\n\\ndelivery_days: How many days did it take for the product to arrive? If this information is not found, output -1.\\n\\nprice_value: Extract any sentences about the value or price,and output them as a comma separated Python list.\\n\\nFormat the output as JSON with the following keys:\\ngift\\ndelivery_days\\nprice_value\\n\\ntext: {text}\\n'}, input_variables=['text'], output_parser=None, partial_variables={}, template='For the following text, extract the following information:\\n\\ngift: Was the item purchased as a gift for someone else? Answer True if yes, False if not or unknown.\\n\\ndelivery_days: How many days did it take for the product to arrive? If this information is not found, output -1.\\n\\nprice_value: Extract any sentences about the value or price,and output them as a comma separated Python list.\\n\\nFormat the output as JSON with the following keys:\\ngift\\ndelivery_days\\nprice_value\\n\\ntext: {text}\\n', template_format='f-string', validate_template=True)}, prompt=PromptTemplate(lc_kwargs={'input_variables': ['text'], 'template': 'For the following text, extract the following information:\\n\\ngift: Was the item purchased as a gift for someone else? Answer True if yes, False if not or unknown.\\n\\ndelivery_days: How many days did it take for the product to arrive? If this information is not found, output -1.\\n\\nprice_value: Extract any sentences about the value or price,and output them as a comma separated Python list.\\n\\nFormat the output as JSON with the following keys:\\ngift\\ndelivery_days\\nprice_value\\n\\ntext: {text}\\n'}, input_variables=['text'], output_parser=None, partial_variables={}, template='For the following text, extract the following information:\\n\\ngift: Was the item purchased as a gift for someone else? Answer True if yes, False if not or unknown.\\n\\ndelivery_days: How many days did it take for the product to arrive? If this information is not found, output -1.\\n\\nprice_value: Extract any sentences about the value or price,and output them as a comma separated Python list.\\n\\nFormat the output as JSON with the following keys:\\ngift\\ndelivery_days\\nprice_value\\n\\ntext: {text}\\n', template_format='f-string', validate_template=True), additional_kwargs={})]\n</code></pre> <pre><code>messages = prompt_template.format_messages(text=customer_review)\nresponse = chat(messages)\nprint(response.content)\n</code></pre> <pre><code>{\n    \"gift\": true,\n    \"delivery_days\": 2,\n    \"price_value\": [\"It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\"]\n}\n</code></pre> <pre><code>type(response.content)\n</code></pre> <pre><code>str\n</code></pre> <pre><code># You will get an error by running this line of code \n# because'gift' is not a dictionary\n# 'gift' is a string\nresponse.content.get('gift')\n</code></pre> <pre><code>---------------------------------------------------------------------------\n\nAttributeError                            Traceback (most recent call last)\n\nCell In[41], line 4\n      1 # You will get an error by running this line of code \n      2 # because'gift' is not a dictionary\n      3 # 'gift' is a string\n----&gt; 4 response.content.get('gift')\n\n\nAttributeError: 'str' object has no attribute 'get'\n</code></pre>"},{"location":"Model_prompt_parser/#parse-the-llm-output-string-into-a-python-dictionary","title":"Parse the LLM output string into a Python dictionary","text":"<pre><code>from langchain.output_parsers import ResponseSchema\nfrom langchain.output_parsers import StructuredOutputParser\n</code></pre> <pre><code>gift_schema = ResponseSchema(name=\"gift\",\n                             description=\"Was the item purchased\\\n                             as a gift for someone else? \\\n                             Answer True if yes,\\\n                             False if not or unknown.\")\ndelivery_days_schema = ResponseSchema(name=\"delivery_days\",\n                                      description=\"How many days\\\n                                      did it take for the product\\\n                                      to arrive? If this \\\n                                      information is not found,\\\n                                      output -1.\")\nprice_value_schema = ResponseSchema(name=\"price_value\",\n                                    description=\"Extract any\\\n                                    sentences about the value or \\\n                                    price, and output them as a \\\n                                    comma separated Python list.\")\n\nresponse_schemas = [gift_schema, \n                    delivery_days_schema,\n                    price_value_schema]\n</code></pre> <pre><code>output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n</code></pre> <pre><code>format_instructions = output_parser.get_format_instructions()\n</code></pre> <pre><code>print(format_instructions)\n</code></pre> <pre><code>The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n\n```json\n{\n    \"gift\": string  // Was the item purchased                             as a gift for someone else?                              Answer True if yes,                             False if not or unknown.\n    \"delivery_days\": string  // How many days                                      did it take for the product                                      to arrive? If this                                       information is not found,                                      output -1.\n    \"price_value\": string  // Extract any                                    sentences about the value or                                     price, and output them as a                                     comma separated Python list.\n}\n```\n</code></pre> <pre><code>review_template_2 = \"\"\"\\\nFor the following text, extract the following information:\n\ngift: Was the item purchased as a gift for someone else? \\\nAnswer True if yes, False if not or unknown.\n\ndelivery_days: How many days did it take for the product\\\nto arrive? If this information is not found, output -1.\n\nprice_value: Extract any sentences about the value or price,\\\nand output them as a comma separated Python list.\n\ntext: {text}\n\n{format_instructions}\n\"\"\"\n\nprompt = ChatPromptTemplate.from_template(template=review_template_2)\n\nmessages = prompt.format_messages(text=customer_review, \n                                format_instructions=format_instructions)\n</code></pre> <pre><code>print(messages[0].content)\n</code></pre> <pre><code>For the following text, extract the following information:\n\ngift: Was the item purchased as a gift for someone else? Answer True if yes, False if not or unknown.\n\ndelivery_days: How many days did it take for the productto arrive? If this information is not found, output -1.\n\nprice_value: Extract any sentences about the value or price,and output them as a comma separated Python list.\n\ntext: This leaf blower is pretty amazing.  It has four settings:candle blower, gentle breeze, windy city, and tornado. It arrived in two days, just in time for my wife's anniversary present. I think my wife liked it so much she was speechless. So far I've been the only one using it, and I've been using it every other morning to clear the leaves on our lawn. It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\n\n\nThe output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n\n```json\n{\n    \"gift\": string  // Was the item purchased                             as a gift for someone else?                              Answer True if yes,                             False if not or unknown.\n    \"delivery_days\": string  // How many days                                      did it take for the product                                      to arrive? If this                                       information is not found,                                      output -1.\n    \"price_value\": string  // Extract any                                    sentences about the value or                                     price, and output them as a                                     comma separated Python list.\n}\n```\n</code></pre> <pre><code>response = chat(messages)\n</code></pre> <pre><code>print(response.content)\n</code></pre> <pre><code>```json\n{\n    \"gift\": true,\n    \"delivery_days\": \"2\",\n    \"price_value\": [\"It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\"]\n}\n```\n</code></pre> <pre><code>output_dict = output_parser.parse(response.content)\n</code></pre> <pre><code>output_dict\n</code></pre> <pre><code>{'gift': True,\n 'delivery_days': '2',\n 'price_value': [\"It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\"]}\n</code></pre> <pre><code>type(output_dict)\n</code></pre> <pre><code>dict\n</code></pre> <pre><code>output_dict.get('delivery_days')\n</code></pre> <pre><code>'2'\n</code></pre>"},{"location":"Question_and_Answer/","title":"LangChain: Q&amp;A over Documents","text":"<p>An example might be a tool that would allow you to query a product catalog for items of interest.</p> <pre><code>#pip install --upgrade langchain\n</code></pre> <pre><code>import os\n\nfrom dotenv import load_dotenv, find_dotenv\n_ = load_dotenv(find_dotenv()) # read local .env file\n</code></pre> <pre><code>from langchain.chains import RetrievalQA\nfrom langchain.chat_models import AzureChatOpenAI\nfrom langchain.document_loaders import CSVLoader\nfrom langchain.vectorstores import DocArrayInMemorySearch\nfrom IPython.display import display, Markdown\nfrom langchain.embeddings import OpenAIEmbeddings\n</code></pre> <pre><code>file = 'OutdoorClothingCatalog_1000.csv'\nloader = CSVLoader(file_path=file, encoding='utf-8')\n</code></pre> <pre><code># Set OpenAI API key\nos.environ[\"OPENAI_API_TYPE\"] = os.getenv(\"api_type\")\nos.environ[\"OPENAI_API_BASE\"] = os.getenv(\"api_base\")\nos.environ[\"OPENAI_API_VERSION\"] = os.getenv(\"api_version\")\nos.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n</code></pre> <pre><code>llm = AzureChatOpenAI(deployment_name=\"chatgpt-gpt35-turbo\",model_name=\"gpt-35-turbo\",temperature=0.0)\n</code></pre> <pre><code>from langchain.indexes import VectorstoreIndexCreator\n</code></pre> <pre><code># pip install docarray\n</code></pre> <pre><code>index = VectorstoreIndexCreator(\n    vectorstore_cls=DocArrayInMemorySearch,\n    embedding = OpenAIEmbeddings(model = \"text-embedding-ada-002\",chunk_size=1)\n).from_loaders([loader])\n</code></pre> <pre><code>query =\"Please list all your shirts with sun protection \\\nin a table in markdown and summarize each one.\"\n</code></pre> <pre><code>response = index.query(query, llm)\n</code></pre> <pre><code>display(Markdown(response))\n</code></pre> Name Description Sun Protection Refresh Swimwear, V-Neck Tankini Contrasts Watersport-ready tankini top designed to move with you and stay comfortable. UPF 50+ rated \u2013 the highest rated sun protection possible. Infant and Toddler Girls' Coastal Chill Swimsuit, Two-Piece Toddler's two-piece swimsuit with bright colors, ruffles, and exclusive whimsical prints. UPF 50+ rated \u2013 the highest rated sun protection possible, blocking 98% of the sun's harmful rays. <p>There is only two shirts with sun protection: - Refresh Swimwear, V-Neck Tankini Contrasts: A watersport-ready tankini top designed to move with you and stay comfortable. It has UPF 50+ rated sun protection. - Infant and Toddler Girls' Coastal Chill Swimsuit, Two-Piece: A toddler's two-piece swimsuit with bright colors, ruffles, and exclusive whimsical prints. It has UPF 50+ rated sun protection, blocking 98% of the sun's harmful rays.</p> <pre><code>loader = CSVLoader(file_path=file, encoding='utf-8')\n</code></pre> <pre><code>docs = loader.load()\n</code></pre> <pre><code>docs[0]\n</code></pre> <pre><code>Document(lc_kwargs={'page_content': \": 0\\nname: Women's Campside Oxfords\\ndescription: This ultracomfortable lace-to-toe Oxford boasts a super-soft canvas, thick cushioning, and quality construction for a broken-in feel from the first time you put them on. \\n\\nSize &amp; Fit: Order regular shoe size. For half sizes not offered, order up to next whole size. \\n\\nSpecs: Approx. weight: 1 lb.1 oz. per pair. \\n\\nConstruction: Soft canvas material for a broken-in feel and look. Comfortable EVA innersole with Cleansport NXT\u00ae antimicrobial odor control. Vintage hunt, fish and camping motif on innersole. Moderate arch contour of innersole. EVA foam midsole for cushioning and support. Chain-tread-inspired molded rubber outsole with modified chain-tread pattern. Imported. \\n\\nQuestions? Please contact us for any inquiries.\", 'metadata': {'source': 'OutdoorClothingCatalog_1000.csv', 'row': 0}}, page_content=\": 0\\nname: Women's Campside Oxfords\\ndescription: This ultracomfortable lace-to-toe Oxford boasts a super-soft canvas, thick cushioning, and quality construction for a broken-in feel from the first time you put them on. \\n\\nSize &amp; Fit: Order regular shoe size. For half sizes not offered, order up to next whole size. \\n\\nSpecs: Approx. weight: 1 lb.1 oz. per pair. \\n\\nConstruction: Soft canvas material for a broken-in feel and look. Comfortable EVA innersole with Cleansport NXT\u00ae antimicrobial odor control. Vintage hunt, fish and camping motif on innersole. Moderate arch contour of innersole. EVA foam midsole for cushioning and support. Chain-tread-inspired molded rubber outsole with modified chain-tread pattern. Imported. \\n\\nQuestions? Please contact us for any inquiries.\", metadata={'source': 'OutdoorClothingCatalog_1000.csv', 'row': 0})\n</code></pre> <pre><code>from langchain.embeddings import OpenAIEmbeddings\nembeddings = OpenAIEmbeddings(model = \"text-embedding-ada-002\",chunk_size=1)\n</code></pre> <pre><code>embed = embeddings.embed_query(\"Hi my name is Harrison\")\n</code></pre> <pre><code>print(len(embed))\n</code></pre> <pre><code>1536\n</code></pre> <pre><code>print(embed[:5])\n</code></pre> <pre><code>[-0.02186359278857708, 0.006734037306159735, -0.01820078119635582, -0.03919587284326553, -0.014047075994312763]\n</code></pre> <pre><code>db = DocArrayInMemorySearch.from_documents(\n    docs, \n    embeddings\n)\n</code></pre> <pre><code>query = \"Please suggest a shirt with sunblocking\"\n</code></pre> <pre><code>docs = db.similarity_search(query)\n</code></pre> <pre><code>len(docs)\n</code></pre> <pre><code>4\n</code></pre> <pre><code>docs[0]\n</code></pre> <pre><code>Document(lc_kwargs={'page_content': \": 5\\nname: Smooth Comfort Check Shirt, Slightly Fitted\\ndescription: Our men's slightly fitted check shirt is the perfect choice for your wardrobe! Customers love how it fits right out of the dryer. Size &amp; Fit: Slightly Fitted, Relaxed through the chest and sleeve with a slightly slimmer waist. Fabric &amp; Care: 100% cotton poplin, with wrinkle-free performance that won't wash out. Our innovative TrueCool\u00ae fabric wicks moisture away from your skin and helps it dry quickly. Additional Features: Traditional styling with a button-down collar and a single patch pocket. Imported.\", 'metadata': {'source': 'OutdoorClothingCatalog_1000.csv', 'row': 5}}, page_content=\": 5\\nname: Smooth Comfort Check Shirt, Slightly Fitted\\ndescription: Our men's slightly fitted check shirt is the perfect choice for your wardrobe! Customers love how it fits right out of the dryer. Size &amp; Fit: Slightly Fitted, Relaxed through the chest and sleeve with a slightly slimmer waist. Fabric &amp; Care: 100% cotton poplin, with wrinkle-free performance that won't wash out. Our innovative TrueCool\u00ae fabric wicks moisture away from your skin and helps it dry quickly. Additional Features: Traditional styling with a button-down collar and a single patch pocket. Imported.\", metadata={'source': 'OutdoorClothingCatalog_1000.csv', 'row': 5})\n</code></pre> <pre><code>retriever = db.as_retriever()\n</code></pre> <pre><code>qdocs = \"\".join([docs[i].page_content for i in range(len(docs))])\n</code></pre> <pre><code>response = llm.call_as_llm(f\"{qdocs} Question: Please list all your \\\nshirts with sun protection in a table in markdown and summarize each one.\") \n</code></pre> <pre><code>display(Markdown(response))\n</code></pre> Shirt Name Sun Protection Summary Refresh Swimwear, V-Neck Tankini Contrasts UPF 50+ This swimtop is made with recycled nylon and Lycra spandex for stretch and breathability. It has a flattering V-neck silhouette and racerback straps. Provides the highest rated sun protection possible. Infant and Toddler Girls' Coastal Chill Swimsuit, Two-Piece UPF 50+ This toddler's two-piece swimsuit has bright colors, ruffles, and exclusive prints. The four-way-stretch and chlorine-resistant fabric keeps its shape and resists snags. The crossover no-slip straps and fully lined bottom ensure a secure fit and maximum coverage. Provides the highest rated sun protection possible. Smooth Comfort Check Shirt, Slightly Fitted N/A This men's check shirt is made with 100% cotton poplin and wrinkle-free performance. It has a slightly fitted, relaxed fit and traditional styling with a button-down collar and patch pocket. EcoFlex 3L Storm Pants N/A These waterproof pants are made with TEK O2 technology for enhanced breathability. They have a slightly fitted fit and are ideal for a variety of outdoor activities year-round. Features include weather-blocking gaiters, side zips, and multiple pockets. <p>Summary: The Refresh Swimwear and Infant and Toddler Girls' Coastal Chill Swimsuit both provide UPF 50+ sun protection, making them ideal for outdoor water activities. The Smooth Comfort Check Shirt and EcoFlex 3L Storm Pants do not have sun protection, but offer other features such as wrinkle-free performance and waterproof protection.</p> <pre><code>qa_stuff = RetrievalQA.from_chain_type(\n    llm=llm, \n    chain_type=\"stuff\", \n    retriever=retriever, \n    verbose=True\n)\n</code></pre> <pre><code>query =  \"Please list all your shirts with sun protection in a table \\\nin markdown and summarize each one.\"\n</code></pre> <pre><code>response = qa_stuff.run(query)\n</code></pre> <pre><code>Error in on_chain_start callback: 'name'\n\n\n\n\u001b[1m&gt; Finished chain.\u001b[0m\n</code></pre> <pre><code>display(Markdown(response))\n</code></pre> Name Description Sun Protection Refresh Swimwear, V-Neck Tankini Contrasts Watersport-ready tankini top designed to move with you and stay comfortable. Made with premium Italian-blend fabric that is breathable, quick-drying, and abrasion-resistant. UPF 50+ rated. UPF 50+ rated Infant and Toddler Girls' Coastal Chill Swimsuit, Two-Piece Two-piece swimsuit for toddlers with bright colors, ruffles, and exclusive whimsical prints. Made with four-way-stretch and chlorine-resistant fabric that keeps its shape and resists snags. UPF 50+ rated. UPF 50+ rated <p>Both the Refresh Swimwear, V-Neck Tankini Contrasts and Infant and Toddler Girls' Coastal Chill Swimsuit, Two-Piece provide UPF 50+ rated sun protection. The Refresh Swimwear is a tankini top designed for watersports, made with premium Italian-blend fabric that is breathable, quick-drying, and abrasion-resistant. The Infant and Toddler Girls' Coastal Chill Swimsuit is a two-piece swimsuit for toddlers with bright colors, ruffles, and exclusive whimsical prints, made with four-way-stretch and chlorine-resistant fabric that keeps its shape and resists snags.</p>"},{"location":"contact/","title":"Contact us","text":"<p>Contributers</p> Ashish kumar Area of expertise : Natural Language Processing, Computer Vision, MLOps,  Generative AI, Statistical Modeling, Uncertainity Analysis, Data Science, Research and Development"}]}